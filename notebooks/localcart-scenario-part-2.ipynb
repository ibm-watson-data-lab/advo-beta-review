{"nbformat": 4, "cells": [{"source": "# LocalCart scenario part 2: Creating a streaming pipeline", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "<a id=\"intro\"></a>\n## Introduction\n\n\nA web or mobile app will trigger events as a user navigates a web site. These clickstream events indicate when a customer logs in, adds something to a basket, completes an order, and logs out. The events are placed into configured Message Hub (Apache Kafka) that provides a scalable way to buffer the data before it is saved, analysed, and rendered. \n\n[Notebook #1 - Creating a Kafka Producer of ClickStream events](https://apsportal.ibm.com/analytics/notebooks/c3aee820-01af-478f-bd0f-07d80866863f/view?projectid=81238e6c-a19b-4c5c-9e45-753dfe7b7de3&context=analytics) generates clickstream events for LocalCart and sends them to Message Hub to show how data can be collected offline and streamed to the cloud later. A [Java app](https://localcartkafkaproducer.mybluemix.net/LocalCartKafkaProducer/) continuously feeds a simulated stream of events to Message Hub. \n\nThis notebook creates a streaming pipeline that ingests those clickstream events, processes each event type, aggregates the events for real-time analysis, and streams them to Redis storage on the cloud for later analysis.\n\nThis notebook runs on Python 2 with Spark 2.0.", "cell_type": "markdown", "metadata": {}}, {"source": "### Static data analysis\n\nIn order to analyse the data at rest, we can create a streaming pipline job that takes the data from Message  Hub and stores it in CSV files. These files can be concatenated and loaded into a Jupyter notebook. We can use [Pixiedust](https://github.com/ibm-cds-labs/pixiedust) to analyse the data. This type of analysis with Pixiedust is done in [Notebook #4: Visualize streaming data](https://apsportal.ibm.com/analytics/notebooks/d9fd6d78-d55f-4e83-b8ae-d465f7af256f/view?projectid=81238e6c-a19b-4c5c-9e45-753dfe7b7de3&context=analytics).\n\n\n### Streaming data analysis\n\nIn order to gather aggregations of each event as it arrives, we can set up a streaming pipeline that performs aggregation operations and writes the output to Redis. Aggregation operations enable us to do some real-time analysis as the data streams.\n\n\n### Visualization\n\nAfter we have aggregations in Redis, we can see the data in dashboards by using Pixiedust in notebooks, or by using custom-made Node.js web applications.  ", "cell_type": "markdown", "metadata": {}}, {"source": "## Table of contents\n\n1. [Introduction](#intro)<br>\n2. [Scenario](#process)<br>\n3. [Collect data from Message Hub](#collect)<br>\n4. [Process the incoming events with Python code](#process_events)<br>\n5. [Set up Aggregation functions for events](#aggregation)<br>\n6. [Provision and set up Redis database](#redis)<br>\n7. [Repeat for next streams ....](#repeat)<br>\n8. [Summary and next steps](#summary)<br>\n", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"process\"></a>\n## Scenario \n\nIn this notebook, our aim is to analyse the incoming events by using the streaming pipelines service. The following graphic shows LocalCart clickstream events that are generated and sent from the Message Hub service. \n<img src='https://github.com/notebookgraphics/advobeta/blob/master/NB2_COMPLETE_STREAMING_PIPELINE.gif?raw=true'></img>\n\n- The first stream deals with the login event type. It is aggregated to get its count.\n- The second stream deals with the add_to_cart event type. It is processed by Python code to compute the total basket price and the UTZ time stamp, and then aggregated to get the sum.\n- The third stream deals with the checkout event type. It is processed by Python code to compute the total basket price and the UTZ time stamp, and then aggregated to get the sum.\n\nWe use the same Redis instance for all three streams in our pipeline.\n", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "<a id=\"collect\"></a>\n## Collect data from Message Hub\n\nFirst we need to create a streaming pipeline that collects data from a Message Hub operator.\n\n\n***\n\n### Steps\n\nIn IBM Data Science Experience, do these steps:\n\n1. Select a project that you want to contain the streaming pipeline.\n1. Click the **Analytics Assets** tab\n1. In the Streaming Pipelines section, click **add streaming piplelines**.\n1. In the Create Streaming Pipeline window, click **Create Manually**. Type in a name for the pipeline, and then click **Continue**.\n1. Drag a MessageHub source operator into the pipeline canvas.\n1. Configure the MessageHub operator by doing these steps in the Properties pane:\n\t1. Select the ClickStream MessageHub instance.\n\t1. Select the topic that you want to aggregate. For example, you might select \"Login\".\n\t1. Click **Edit Schema** to match the incoming data:\n        - `customer_id` - text - `.customer_id`\n        - `click_event_type` - text - `.click_event_type`\n        - `event_time` - text - `.event_time`\n\n\nOur streaming pipeline now has its first operator and looks like this: <img src='https://github.com/notebookgraphics/advobeta/blob/master/NB2_MH.gif?raw=true'></img>", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"process_events\"></a>\n## Process the incoming events with Python code", "cell_type": "markdown", "metadata": {}}, {"source": "Python code can be used to to process incoming events. The Python code has the following form:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 47, "source": "def process(event):\n    return {'output':'output'}", "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"source": "With each incoming event, the streaming pipeline will call \"process\" with the incoming data. Whatever your function returns will be sent to the next block. Here is an example of a login clickstream event:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 48, "source": "ev = { \n    'customer_id': '14420', \n    'click_event_type': 'login',     \n    'total_price_of_basket' : \"0.0\",\n    'total_number_of_items_in_basket' : \"0\",\n    'total_number_of_distinct_items_in_basket' : \"0\",\n    'event_time': '2017-04-10 15:54:35 IST'\n}", "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"source": "Let's make a \"process\" function that parses the time stamp and returns the parsed date:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 51, "source": "from dateutil.parser import parse\nfrom dateutil import tz\ndef process(event):\n    # datetime.parse doesn't understant \"IST\" as a timezone indicator, so swap for +05:30\n    dt = parse(event['event_time'].replace('IST','+05:30'))\n    \n    # convert to UTC timezone too\n    event['dt_utc'] = dt.astimezone(tz.gettz('UTC'))\n\n    return event", "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"source": "and convert our sample data with it:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 52, "source": "print process(ev)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "{'event_time': '2017-04-10 15:54:35 IST', 'total_number_of_items_in_basket': '0', 'dt_utc': datetime.datetime(2017, 4, 10, 10, 24, 35, tzinfo=tzfile('/usr/share/zoneinfo/UTC')), 'total_number_of_distinct_items_in_basket': '0', 'total_price_of_basket': '0.0', 'click_event_type': 'login', 'x': 'hello', 'customer_id': '14420'}\n"}]}, {"source": "***\n\n### Steps\n\nIn the pipeline canvas, do these steps:\n1. Drag a Code operator from the Processing and Analytics area, and then drop it on the canvas next to the MessageHub operator.\n2. Drag your mouse pointer from the output port of the MessageHub operator to the input port of the Code operator to connect them.\n3. Click the **Code** operator to open its Properties pane. \n    - Copy and paste the following code into the Code block:\n\n    ```\n        from dateutil.parser import parse\n        from dateutil import tz\n        def process(event):\n            # datetime.parse doesn't understant \"IST\" as a timezone indicator, so swap for +05:30\n            dt = parse(event['event_time'].replace('IST','+05:30'))\n    \n            # convert to UTC timezone too\n            event['dt_utc'] = dt.astimezone(tz.gettz('UTC'))\n\n        return event\n    ```\n\n   - Click **Edit Schema** to edit the code block to match the schema in the MessageHub operator. Add a new attribute to the Code's schema: `dt_utc` which is of type Date.\n\n\nYou now have a streaming pipeline with two operators and that looks like this:\n<img src='https://github.com/notebookgraphics/advobeta/blob/master/NB2_MH_CODE.gif?raw=true'></img>\n\n\n\nLet's turn now to the third operator, Aggregation.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"aggregation\"></a>\n## Set up Aggregation functions for events\n\nStreaming data can be aggregated and then a function such as sum, count, minimum, or maximim can be done on the aggregation before it is written to the Redis database. Our aim is to calculate the following data for a sliding one-hour window:\n\n- login_count - the number of people who logged into LocalCart \n- basket_count - the number of items added into a shopping cart\n- checkout_count - the number of purchases made\n- basket_total - the total price of items added into a shopping cart\n- checkout_total - the total price of items purchased\n\nLet's walk though how to assign an aggregation function for the `login_count` event type in our streaming data.\n\n***\n\n### Steps\n\nIn the pipeline canvas, do these steps:\n\n- Drag a Aggregation operator from the Processing and Analytics area, and then drop it on the canvas next to the Code operator.\n- Drag your mouse pointer from the output port of the Code operator to the input port of the Aggregation operator to connect them.\n- Click the Aggregation operator to open its Properties pane. Set the following parameters:\n    - Type - 'sliding'\n    - Time Units - 'hour'\n    - Number of Time Units - 1\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n- In the Functions area of the Aggregation Properties pane, assign the following values:\n    - Output Field Name - `login_count`\n    - Function Type - count\n    - Apply Function To - `login_count` \n     ***\n     \n\nRepeat the steps above for the `basket_count` and `checkout_count` aggregators on their respective Message Hub topics.\n    \nThe `basket_total` and `checkout_total` aggregations are achieved by adding a second aggregation function to the existing block, this time using a \"Sum\" function.\n\n\nOur pipeline now has three operators and looks like this:\n<img src='https://github.com/notebookgraphics/advobeta/blob/master/NB2_MH_CODE_AGGREG.gif?raw=true'></img>\n\n", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"redis\"></a>\n## Provision and set up Redis database\n\nRedis is an in-memory database. It stores its data in RAM, making it a very fast way of storing and retrieving data. It provides a set of primitive data structures, but we only concern ourselves with [hashes](https://redis.io/commands#hash) for this exercise.\n\nA Redis hash is a data structure that allows several keys to be stored together. We are going to configure a Redis hash called \"funnel\" that contains the following output:\n\n- login_count - the number of people who logged into LocalCart\n- basket_count - the number of items added into a shopping cart\n- checkout_count - the number of purchases made\n- basket_total - the total price of items added into a shopping cart\n- checkout_total - the total price of items purchased\n\nThese are the outputs of the aggregation functions in our streaming pipeline. Let's provision our own Redis service:\n\n***\n\n### Steps\n\nIn the IBM Bluemix Dashboard, do these steps:\n\n1. Click the **Services** tab.\n1. Choose the **Redis By Compose** service.\n1. Provision a new Redis By Compose service. Note its authentication details (hostname, port, and password).\n\n***", "cell_type": "markdown", "metadata": {}}, {"source": "Now we can play with the Redis service in this notebook by installing the Python Redis library with the following command:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "source": "!pip install redis", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Requirement already satisfied: redis in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bf-6467be2fbf2236-f5ca97461bb5/.local/lib/python2.7/site-packages\r\n"}]}, {"source": "We import the library with this command:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 37, "source": "import redis\nr = redis.StrictRedis(host='abc.com', port=10115, db=0, password='ABCDEFGHI')", "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"source": "We can then create a hash called 'funnel' to store our real-time data to the database by using the `hset` function:\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "source": "r.hset('funnel', 'basket_count', 554);\nr.hset('funnel', 'basket_total', 951);\nr.hset('funnel', 'checkout_count', 21);\nr.hset('funnel', 'checkout_total', 5400);\nr.hset('funnel', 'login_count', 100);", "cell_type": "code", "metadata": {}, "outputs": [{"evalue": "name 'r' is not defined", "traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-1-f56bd6854919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funnel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'basket_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m554\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funnel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'basket_total'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m951\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funnel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkout_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funnel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkout_total'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funnel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'login_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"], "output_type": "error", "ename": "NameError"}]}, {"source": "We can also use this connection to retrieve all the values from our 'funnel' hash using `hgetall`:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "source": "r.hgetall('funnel')", "cell_type": "code", "metadata": {}, "outputs": [{"evalue": "name 'r' is not defined", "traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-1-fdedc9c43f40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhgetall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funnel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"], "output_type": "error", "ename": "NameError"}]}, {"source": "**Note:** \nThe Redis connection above seems to freeze in this notebook after a minute or so. In this case, you will need to restart the notebook kernel to restore it.\n", "cell_type": "markdown", "metadata": {}}, {"source": "We can now store the aggregations from our streaming pipeline in Redis.", "cell_type": "markdown", "metadata": {}}, {"source": "------------------------------------------------------------------------------\n\n### Steps\n\nIn the Bluemix Dashboard, do these steps:\n\n1. Click the **Services** tab.\n1. Choose the **Redis By Compose** service.\n1. Provision a new Redis By Compose service. Note its authentication details (hostname, port, and password).\n\nLet's add a Redis output to our streaming pipelines. In the streaming pipeline canvas, do these steps:\n\n1. Drag a Redis operator from the Target area, and then drop it on the canvas next to the Aggregator operator.\n1. Drag your mouse pointer from the output port of the Aggregator operator to the input port of the Redis operator to connect them.\n2. Click the **Redis** operator to open its Properties pane. \n    - Type in the value of 'hostname', 'port', and 'password' of your Redis by Compose service.\n    - In the Key Template field, type in \"funnel\". \n\n\n***\n\nYour pipeline should now look like this:\n<img src='https://github.com/notebookgraphics/advobeta/blob/master/NB2_STREAMING_PIPELINE.gif?raw=true'></img>\n\n\nCongratulations! You just created a streaming pipeline that takes clickstream data from MessageHub, processes the data with Python, aggregates the event types and applies functions to them, and then writes counts and totals to Redis storage.\n", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"repeat\"></a>\n## and repeat for next streams ....\n\nYou now need to create two more streams into the pipeline. One stream is for the \"add_to_cart\" Message Hub event type. The second stream is for the \"checkout\" Message Hub topic. The messages on those topics are a bit more detailed:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "source": "add_to_cart_event = {\n    \"customer_id\": \"13859\",\n    \"click_event_type\": \"add_to_cart\",\n    \"product_name\": \"Oatmeal\",\n    \"product_category\": \"Food\",\n    \"product_price\": \"2.49\",\n    \"total_price_of_basket\": \"153.41\",\n    \"total_number_of_items_in_basket\": \"19\",\n    \"total_number_of_distinct_items_in_basket\": \"6\",\n    \"event_time\": \"2017-06-23 12:56:18 UTC\"\n}\ncheckout_event =  {\n    \"customer_id\": \"11828\",\n    \"click_event_type\": \"checkout\",\n    \"total_price_of_basket\": \"72.80000000000001\",\n    \"total_number_of_items_in_basket\": \"20\",\n    \"total_number_of_distinct_items_in_basket\": \"5\",\n    \"session_duration\": \"440\",\n    \"event_time\": \"2017-06-23 13:09:12 UTC\"\n}", "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"source": "When you create those two streams, you need to add extra fields to the Message Hub schema and parse them correctly in the Python code.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": null, "source": "from dateutil.parser import parse\nfrom dateutil import tz\ndef process(event):\n    # datetime.parse doesn't understant \"IST\" as a timezone indicator, so swap for +05:30\n    dt = parse(event['event_time'].replace('IST','+05:30'))\n    \n    # convert to UTC timezone too\n    event['dt_utc'] = dt.astimezone(tz.gettz('UTC'))\n    event['total_price_of_basket'] = float(event['total_price_of_basket'])\n    return event\n", "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"source": "\nDrag a second Aggregation operator to the canvas, connect it to the Code operator for the add_to_cart event type, and define the Aggregator Properties pane with the following values:\n    - Type - 'sliding'\n    - Time Units - 'hour'\n    - Number of Time Units - 1\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n    - Output Field Name - `basket_total`\n    - Function Type - sum\n    - Apply Function To - `basket_total` \n\n\nDrag a third Aggregation operator to the canvas, connect it to the Code operator for checkout event type, and define the Aggregator Properties pane with the following values:\n    - Type - 'sliding'\n    - Time Units - 'hour'\n    - Number of Time Units - 1\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n    - Output Field Name - `checkout_total`\n    - Function Type - sum\n    - Apply Function To - `checkout_total` \n\n\nWe use the same Redis instance for all three streams in our pipeline. Consequently, you only need to create the Redis By Compose service one time.\n\nOur complete streaming pipeline now looks like this: \n\n<img src='https://github.com/notebookgraphics/advobeta/blob/master/NB2_COMPLETE_STREAMING_PIPELINE.gif?raw=true'></img>", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"summary\"></a>\n\n## Summary and next steps\nIn this notebook you set up a streaming pipeline that used data from notebook [Notebook #1: Creating a Kafka Producer of ClickStream events](https://apsportal.ibm.com/analytics/notebooks/c3aee820-01af-478f-bd0f-07d80866863f/view?projectid=81238e6c-a19b-4c5c-9e45-753dfe7b7de3&context=analytics). The data included clickstream events (log in, browse, add to cart, logout without purchase, logout with purchase, and checkout).  \n\n### Author\nGlynn Bird is a Developer Advocate for Watson Data Platform at IBM. \n\n***\n\nCopyright \u00a9 IBM Corp. 2017. This notebook and its source code are released under the terms of the MIT License.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "source": "", "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}], "metadata": {"language_info": {"pygments_lexer": "ipython2", "version": "2.7.11", "mimetype": "text/x-python", "name": "python", "file_extension": ".py", "codemirror_mode": {"version": 2, "name": "ipython"}, "nbconvert_exporter": "python"}, "kernelspec": {"language": "python", "display_name": "Python 2 with Spark 2.0", "name": "python2-spark20"}}, "nbformat_minor": 1}