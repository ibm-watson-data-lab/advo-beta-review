{"metadata": {"language_info": {"name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "pygments_lexer": "ipython2", "codemirror_mode": {"name": "ipython", "version": 2}, "file_extension": ".py", "version": "2.7.11"}, "kernelspec": {"display_name": "Python 2 with Spark 2.0", "name": "python2-spark20", "language": "python"}}, "nbformat_minor": 1, "cells": [{"metadata": {"collapsed": true}, "source": "# LocalCart scenario part 2: Creating streaming pipelines", "cell_type": "markdown"}, {"metadata": {}, "source": "This notebook contains two examples of creating streaming pipelines:\n\n- [Example 1: Creating a Message Hub to Redis streaming pipeline that uses Code and Aggregation operators](#intro_a)<br>\n- [Example 2: Creating a Message Hub to Object Storage streaming pipeline](#intro_b)<br>\n\nThis notebook runs on Python 2 with Spark 2.0.", "cell_type": "markdown"}, {"metadata": {}, "source": "<a id=\"intro_a\"></a>\n***\n\n# Example 1: Creating a Message Hub to Redis streaming pipeline that uses Code and Aggregation operators\n\n***\n\n\n## Introduction \n\n\nA web or mobile app will trigger events as a user navigates a web site. These clickstream events indicate when a customer logs in, adds something to a basket, completes an order, and logs out. The events are placed into configured Message Hub (Apache Kafka) that provides a scalable way to buffer the data before it is saved, analysed, and rendered. \n\n[Notebook #1 - Creating a Kafka Producer of ClickStream events](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-1.ipynb) generates clickstream events for LocalCart and sends them to Message Hub to show how data can be collected offline and streamed to the cloud later. A [Java app](https://localcartkafkaproducer.mybluemix.net/LocalCartKafkaProducer/) continuously feeds a simulated stream of events to Message Hub. \n\nThis example creates a streaming pipeline that ingests those clickstream events, processes each event type, aggregates the events for real-time analysis, and streams them to Redis storage on the cloud for later analysis. \n\n", "cell_type": "markdown"}, {"metadata": {}, "source": "### Static data analysis\n\nIn order to analyse the data at rest, we can create a streaming pipeline job that takes the data from Message Hub and stores it in CSV files. This process is described in [Example 2: MessageHub to CSV streaming pipelines](#intro_b) These files can be concatenated and loaded into a Jupyter notebook. We can use [PixieDust](https://github.com/ibm-cds-labs/pixiedust) to analyse the data. This type of analysis with PixieDust is done in [Notebook #4: Visualize streaming data](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-4.ipynb).\n\n\n### Streaming data analysis\n\nIn order to gather aggregations of each event as it arrives, we can set up a streaming pipeline that performs aggregation operations and writes the output to Redis. Aggregation operations enable us to do some real-time analysis as the data streams.\n\n\n### Visualization\n\nAfter we have aggregations in Redis, we can see the data in dashboards by using PixieDust in Jupyter notebooks, or by using custom-made Node.js web applications.  ", "cell_type": "markdown"}, {"metadata": {}, "source": "## Table of contents\n\n1. [Introduction](#intro_a)<br>\n2. [Scenario](#process_a)<br>\n3. [Collect data from Message Hub](#collect_a)<br>\n4. [Process the incoming events with Python code](#process_events)<br>\n5. [Set up aggregation functions for events](#aggregation)<br>\n6. [Provision and set up Redis database](#redis)<br>\n7. [Repeat for next streams ....](#repeat)<br>\n8. [Summary and next steps](#summary)<br>\n", "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "source": "<a id=\"process_a\"></a>\n## Scenario\n\nIn this notebook, our aim is to analyse the incoming events by using the streaming pipelines service. The following graphic shows LocalCart clickstream events that are generated and sent from the Message Hub service. \n<img src='https://github.com/notebookgraphics/advobeta/blob/master/NB2_COMPLETE_STREAMING_PIPELINE.gif?raw=true'></img>\n\n- The first stream deals with the login event type. It is aggregated to get its count.\n- The second stream deals with the add_to_cart event type. It is processed by Python code to compute the total basket price and the UTZ time stamp, and then aggregated to get the sum.\n- The third stream deals with the checkout event type. It is processed by Python code to compute the total basket price and the UTZ time stamp, and then aggregated to get the sum.\n\nWe use the same Redis instance for all three streams in our pipeline.\n", "cell_type": "markdown"}, {"metadata": {}, "source": "<a id=\"collect_a\"></a>\n## Collect data from Message Hub\n\nFirst we need to create a streaming pipeline that collects data from a Message Hub operator.\n\n\n***\n\n### Steps\n\nIn IBM Data Science Experience, do these steps:\n\n1. Select a project that you want to contain the streaming pipeline.\n1. Click the **Analytics Assets** tab\n1. In the Streaming Pipelines section, click **add streaming piplelines**.\n1. In the Create New Streaming Pipeline window, click **Manual**. Type in a name for the pipeline, and then click **Create**.\n1. Drag a MessageHub source operator into the pipeline canvas.\n1. Configure the MessageHub operator by doing these steps in the Properties pane:\n\t1. Select the ClickStream MessageHub instance.\n\t1. Select the topic that you want to aggregate. For example, you might select \"Login\".\n\t1. Click **Edit Schema** to match the incoming data:\n        - `customer_id` - text - `.customer_id`\n        - `click_event_type` - text - `.click_event_type`\n        - `event_time` - text - `.event_time`\n\n\nOur streaming pipeline now has its first operator and looks like this: <img src='https://github.com/notebookgraphics/advobeta/blob/master/NB2_MH.gif?raw=true'></img>", "cell_type": "markdown"}, {"metadata": {}, "source": "<a id=\"process_events\"></a>\n## Process the incoming events with Python code", "cell_type": "markdown"}, {"metadata": {}, "source": "Python code can be used to to process incoming events. The Python code has the following form:", "cell_type": "markdown"}, {"outputs": [], "metadata": {"collapsed": true}, "source": "def process(event):\n    return {'output':'output'}", "cell_type": "code", "execution_count": 4}, {"metadata": {}, "source": "With each incoming event, the streaming pipeline will call \"process\" with the incoming data. Whatever your function returns will be sent to the next block. Here is an example of a login clickstream event:", "cell_type": "markdown"}, {"outputs": [], "metadata": {"collapsed": true}, "source": "ev = { \n    'customer_id': '14420', \n    'click_event_type': 'login',     \n    'total_price_of_basket' : \"0.0\",\n    'total_number_of_items_in_basket' : \"0\",\n    'total_number_of_distinct_items_in_basket' : \"0\",\n    'event_time': '2017-04-10 15:54:35 IST'\n}", "cell_type": "code", "execution_count": 3}, {"metadata": {}, "source": "Let's make a \"process\" function that parses the time stamp and returns the parsed date:", "cell_type": "markdown"}, {"outputs": [], "metadata": {"collapsed": true}, "source": "from dateutil.parser import parse\nfrom dateutil import tz\ndef process(event):\n    # datetime.parse doesn't understand \"IST\" as a timezone indicator, so swap for +05:30\n    dt = parse(event['event_time'].replace('IST','+05:30'))\n    \n    # convert to UTC timezone too\n    event['dt_utc'] = dt.astimezone(tz.gettz('UTC'))\n\n    return event", "cell_type": "code", "execution_count": 2}, {"metadata": {}, "source": "and convert our sample data with it:", "cell_type": "markdown"}, {"outputs": [{"traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-1-913287cf2f79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'process' is not defined"], "output_type": "error", "evalue": "name 'process' is not defined", "ename": "NameError"}], "metadata": {"scrolled": true}, "source": "print process(ev)", "cell_type": "code", "execution_count": 1}, {"metadata": {}, "source": "\n### Steps\n\nIn the pipeline canvas, do these steps:\n1. Drag a Code operator from the Processing and Analytics area, and then drop it on the canvas next to the MessageHub operator.\n2. Drag your mouse pointer from the output port of the MessageHub operator to the input port of the Code operator to connect them.\n3. Click the **Code** operator to open its Properties pane. \n    - Copy and paste the following code into the Code block:\n\n    ```\n        from dateutil.parser import parse\n        from dateutil import tz\n        def process(event):\n            # datetime.parse doesn't understand \"IST\" as a timezone indicator, so swap for +05:30\n            dt = parse(event['event_time'].replace('IST','+05:30'))\n    \n            # convert to UTC timezone too\n            event['dt_utc'] = dt.astimezone(tz.gettz('UTC'))\n\n        return event\n    ```\n\n   - Click **Edit Schema** to edit the code block to match the schema in the MessageHub operator. Add a new attribute to the Code's schema: `dt_utc` which is of type Date.\n\n\nYou now have a streaming pipeline with two operators and that looks like this:\n<img src='https://github.com/notebookgraphics/advobeta/blob/master/NB2_MH_CODE.gif?raw=true'></img>\n\n\n\nLet's turn now to the third operator, Aggregation.", "cell_type": "markdown"}, {"metadata": {}, "source": "<a id=\"aggregation\"></a>\n## Set up aggregation functions for events\n\nStreaming data can be aggregated and then a function such as sum, count, minimum, or maximim can be done on the aggregation before it is written to the Redis database. Our aim is to calculate the following data for a sliding one-hour window:\n\n- login_count - the number of people who logged into LocalCart \n- basket_count - the number of items added into a shopping cart\n- checkout_count - the number of purchases made\n- basket_total - the total price of items added into a shopping cart\n- checkout_total - the total price of items purchased\n\nLet's walk through how to assign an aggregation function for the `login_count` event type in our streaming data.\n\n\n\n### Steps\n\nIn the pipeline canvas, do these steps:\n\n1. Drag a Aggregation operator from the Processing and Analytics area, and then drop it on the canvas next to the Code operator.\n2. Drag your mouse pointer from the output port of the Code operator to the input port of the Aggregation operator to connect them.\n3. Click the Aggregation operator to open its Properties pane. Set the following parameters:\n    - Type - 'sliding'\n    - Time Units - 'hour'\n    - Number of Time Units - 1\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n4. In the Functions area of the Aggregation Properties pane, assign the following values:\n    - Output Field Name - `login_count`\n    - Function Type - count\n    - Apply Function To - `login_count` \n     ***\n     \n\nRepeat the steps above for the `basket_count` and `checkout_count` Aggregation operators on their respective Message Hub topics.\n    \nThe `basket_total` and `checkout_total` aggregations are achieved by adding a second aggregation function to the existing block, this time using a \"Sum\" function.\n\n\nOur pipeline now has three operators and looks like this:\n<img src='https://github.com/notebookgraphics/advobeta/blob/master/NB2_MH_CODE_AGGREG.gif?raw=true'></img>\n\n", "cell_type": "markdown"}, {"metadata": {}, "source": "<a id=\"redis\"></a>\n## Provision and set up Redis database\n\nRedis is an in-memory database. It stores its data in RAM, making it a very fast way of storing and retrieving data. It provides a set of primitive data structures, but we only concern ourselves with [hashes](https://redis.io/commands#hash) for this exercise.\n\nA Redis hash is a data structure that allows several keys to be stored together. We are going to configure a Redis hash called \"funnel\" that contains the following output:\n\n- login_count - the number of people who logged into LocalCart\n- basket_count - the number of items added into a shopping cart\n- checkout_count - the number of purchases made\n- basket_total - the total price of items added into a shopping cart\n- checkout_total - the total price of items purchased\n\nThese are the outputs of the aggregation functions in our streaming pipeline. Let's provision our own Redis service:\n\n\n\n### Steps\n\nIn the IBM Bluemix Dashboard, do these steps:\n\n1. Click the **Services** tab.\n1. Choose the **Redis By Compose** service.\n1. Provision a new Redis By Compose service. Note its authentication details (hostname, port, and password).\n\n", "cell_type": "markdown"}, {"metadata": {}, "source": "Now we can play with the Redis service in this notebook by installing the Python Redis library with the following command:", "cell_type": "markdown"}, {"outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: redis in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s8bf-6467be2fbf2236-f5ca97461bb5/.local/lib/python2.7/site-packages\r\n"}], "metadata": {}, "source": "!pip install redis", "cell_type": "code", "execution_count": 26}, {"metadata": {}, "source": "We import the library with this command:", "cell_type": "markdown"}, {"outputs": [], "metadata": {"collapsed": true}, "source": "import redis\nr = redis.StrictRedis(host='abc.com', port=10115, db=0, password='ABCDEFGHI')", "cell_type": "code", "execution_count": 37}, {"metadata": {}, "source": "We can then create a hash called 'funnel' to store our real-time data to the database by using the `hset` function:\n", "cell_type": "markdown"}, {"outputs": [{"traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-5-f56bd6854919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funnel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'basket_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m554\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funnel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'basket_total'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m951\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funnel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkout_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funnel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkout_total'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funnel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'login_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"], "output_type": "error", "evalue": "name 'r' is not defined", "ename": "NameError"}], "metadata": {}, "source": "r.hset('funnel', 'basket_count', 554);\nr.hset('funnel', 'basket_total', 951);\nr.hset('funnel', 'checkout_count', 21);\nr.hset('funnel', 'checkout_total', 5400);\nr.hset('funnel', 'login_count', 100);", "cell_type": "code", "execution_count": 5}, {"metadata": {}, "source": "We can also use this connection to retrieve all the values from our 'funnel' hash using `hgetall`:", "cell_type": "markdown"}, {"outputs": [{"traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-6-fdedc9c43f40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhgetall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funnel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"], "output_type": "error", "evalue": "name 'r' is not defined", "ename": "NameError"}], "metadata": {}, "source": "r.hgetall('funnel')", "cell_type": "code", "execution_count": 6}, {"metadata": {}, "source": "**Note:** \nThe Redis connection above seems to freeze in this notebook after a minute or so. In this case, you will need to restart the notebook kernel to restore it.\n", "cell_type": "markdown"}, {"metadata": {}, "source": "We can now store the aggregations from our streaming pipeline in Redis.", "cell_type": "markdown"}, {"metadata": {}, "source": "\n### Steps\n\nIn the Bluemix Dashboard, do these steps:\n\n1. Click the **Services** tab.\n1. Choose the **Redis By Compose** service.\n1. Provision a new Redis By Compose service. Note its authentication details (hostname, port, and password).\n\nLet's add a Redis output to our streaming pipelines. In the streaming pipeline canvas, do these steps:\n\n1. Drag a Redis operator from the Target area, and then drop it on the canvas next to the Aggregation operator.\n1. Drag your mouse pointer from the output port of the Aggregation operator to the input port of the Redis operator to connect them.\n2. Click the **Redis** operator to open its Properties pane. \n    - Type in the value of 'hostname', 'port', and 'password' of your Redis by Compose service.\n    - In the Key Template field, type in \"funnel\". \n\n\n***\n\nYour pipeline should now look like this:\n<img src='https://github.com/notebookgraphics/advobeta/blob/master/NB2_STREAMING_PIPELINE.gif?raw=true'></img>\n\n\nCongratulations! You just created a streaming pipeline that takes clickstream data from MessageHub, processes the data with Python, aggregates the event types and applies functions to them, and then writes counts and totals to Redis storage.\n", "cell_type": "markdown"}, {"metadata": {}, "source": "<a id=\"repeat\"></a>\n## and repeat for next streams ....\n\nYou now need to create two more streams into the pipeline. One stream is for the \"add_to_cart\" Message Hub event type. The second stream is for the \"checkout\" Message Hub topic. The messages on those topics are a bit more detailed:", "cell_type": "markdown"}, {"outputs": [], "metadata": {"collapsed": true}, "source": "add_to_cart_event = {\n    \"customer_id\": \"13859\",\n    \"click_event_type\": \"add_to_cart\",\n    \"product_name\": \"Oatmeal\",\n    \"product_category\": \"Food\",\n    \"product_price\": \"2.49\",\n    \"total_price_of_basket\": \"153.41\",\n    \"total_number_of_items_in_basket\": \"19\",\n    \"total_number_of_distinct_items_in_basket\": \"6\",\n    \"event_time\": \"2017-06-23 12:56:18 UTC\"\n}\ncheckout_event =  {\n    \"customer_id\": \"11828\",\n    \"click_event_type\": \"checkout\",\n    \"total_price_of_basket\": \"72.80000000000001\",\n    \"total_number_of_items_in_basket\": \"20\",\n    \"total_number_of_distinct_items_in_basket\": \"5\",\n    \"session_duration\": \"440\",\n    \"event_time\": \"2017-06-23 13:09:12 UTC\"\n}", "cell_type": "code", "execution_count": 19}, {"metadata": {"collapsed": true}, "source": "When you create those two streams, you need to add extra fields to the Message Hub schema and parse them correctly in the Python code.", "cell_type": "markdown"}, {"outputs": [], "metadata": {"collapsed": true}, "source": "from dateutil.parser import parse\nfrom dateutil import tz\ndef process(event):\n    # datetime.parse doesn't understand \"IST\" as a timezone indicator, so swap for +05:30\n    dt = parse(event['event_time'].replace('IST','+05:30'))\n    \n    # convert to UTC timezone too\n    event['dt_utc'] = dt.astimezone(tz.gettz('UTC'))\n    event['total_price_of_basket'] = float(event['total_price_of_basket'])\n    return event\n", "cell_type": "code", "execution_count": 7}, {"metadata": {}, "source": "\nDrag a second Aggregation operator to the canvas, connect it to the Code operator for the add_to_cart event type, and define the Aggregation Properties pane with the following values:\n    - Type - 'sliding'\n    - Time Units - 'hour'\n    - Number of Time Units - 1\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n    - Output Field Name - `basket_total`\n    - Function Type - sum\n    - Apply Function To - `basket_total` \n\n\nDrag a third Aggregation operator to the canvas, connect it to the Code operator for checkout event type, and define the Aggregation Properties pane with the following values:\n    - Type - 'sliding'\n    - Time Units - 'hour'\n    - Number of Time Units - 1\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n    - Output Field Name - `checkout_total`\n    - Function Type - sum\n    - Apply Function To - `checkout_total` \n\n\nWe use the same Redis instance for all three streams in our pipeline. Consequently, you only need to create the Redis By Compose service one time.\n\nOur complete streaming pipeline now looks like this: \n\n<img src='https://github.com/notebookgraphics/advobeta/blob/master/NB2_COMPLETE_STREAMING_PIPELINE.gif?raw=true'></img>", "cell_type": "markdown"}, {"metadata": {}, "source": "<a id=\"summary\"></a>\n\n## Summary and next steps\nIn this section, you set up a streaming pipeline that used data from notebook [Notebook #1: Creating a Kafka Producer of ClickStream events](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-1.ipynb). The data included clickstream events (log in, browse, add to cart, logout without purchase, logout with purchase, and checkout).  \n\n", "cell_type": "markdown"}, {"metadata": {}, "source": "<a id=\"intro_b\"></a>\n\n***\n# Example 2: Creating a Message Hub to Object Storage streaming pipeline\n***\n\n\n## Introduction\n\n\nA web or mobile app will trigger events as a user navigates a web site. These clickstream events indicate when a customer logs in, adds something to a basket, completes an order, and logs out. The events are placed into configured Message Hub (Apache Kafka) that provides a scalable way to buffer the data before it is saved, analysed, and rendered. \n\n[Notebook #1: Creating a Kafka Producer of ClickStream events](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-1.ipynb) generates clickstream events for LocalCart and sends them to Message Hub to show how data can be collected offline and streamed to the cloud later. A [Java app](https://localcartkafkaproducer.mybluemix.net/LocalCartKafkaProducer/) continuously feeds a simulated stream of events to Message Hub. \n\nThis section creates streaming pipelines that ingest those clickstream events, and writes them to CSV format on Object Storage for later analysis.\n\nThese files can be concatenated and loaded into a Jupyter notebook. We can use [PixieDust](https://github.com/ibm-cds-labs/pixiedust) to analyse the data. This type of analysis with PixieDust is done in [Notebook #4: Visualize streaming data](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-4.ipynb).\n\n", "cell_type": "markdown"}, {"metadata": {}, "source": "## Table of contents\n\n1. [Introduction](#intro_b)<br>\n2. [Scenario](#process_b)<br>\n3. [Steps to collect data from Message Hub](#collect_b)<br>\n4. [Summary and next steps](#summary_b)<br>\n   ", "cell_type": "markdown"}, {"metadata": {}, "source": "<a id=\"process_b\"></a>\n## Scenario \n\nIn this notebook, our aim is to persist the incoming events as CSV files by using the streaming pipelines service. The following graphic shows LocalCart clickstream events that are generated and sent from the Message Hub service. \n<img src='https://github.com/ibm-watson-data-lab/advo-beta-producer/blob/master/graphics/NB2a_CSV_PIPELINE.png?raw=true'></img>\n", "cell_type": "markdown"}, {"metadata": {}, "source": "<a id=\"collect_b\"></a>\n## Steps to collect data from Message Hub\n\nFirst, we need to create streaming pipelines that capture clickstream data from a Message Hub operator. Each pipeline will stream data of one clickstream event type to its own CSV file in Object Storage. All CSV files are in the same instance of Object Storage.\n\n\nIn IBM Data Science Experience, do these steps:\n\n1. Select a project that you want to contain the streaming pipeline.\n1. Click the **Analytics Assets** tab\n1. In the Streaming Pipelines section, click **add streaming pipelines**.\n1. In the Create New Streaming Pipeline window, type in a name for the pipeline such as **addtocart2csv**. Click **Wizard**, and then click **Create**. \n1. In the Select Source window, click **MessageHub**.\n1. Under the Instance drop-down menu, select your MessageHub instance.\n1. Under the Topic drop-down menu, select **add_to_cart**. Click **Continue**.\n1. Wait for the Data Preview window to load the streaming data. Click **Continue**.\n1. In the Select Target window, click **Object Storage**.\n1. Under the Object Storage Instance drop-down menu, select the instance that is used by the DSX project.\n   <br>\n   > Take note of the  Object Storage instance name. You will need this information in [Notebook 3b: Static clickstream analysis](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-3b.ipynb) when you load and analyze the clickstream events.\n1. Under the Container drop-down menu, select the Object Storage container you want to write to. \n   <br>\n   > Take note of the  Object Storage container name. You will need this information in [Notebook 3b: Static clickstream analysis](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-3b.ipynb) when you load and analyze the clickstream events.\n1. Under File Name, type **add_to_cart-TIMESTAMP.csv** (**Note:** \"TIMESTAMP\" is a reserved word that will be replaced with an actual timestamp when the file is written).\n1. Under Format, select **csv**.\n1. Under Delimiter, select **Comma (,)**.\n1. Click **Save**. The pipeline is created for you, but it will be in \"Stopped\" state.\n1. In the next window, click the **Run** icon to start the streaming pipeline.\n \n\nNext, repeat the steps above for each Message Hub topic: browsing, checkout, clickstream, login, logout_with_purchase, and logout_without_purchase.\n", "cell_type": "markdown"}, {"metadata": {}, "source": "<a id=\"summary_b\"></a>\n## Summary and next steps\nIn this section, you created seven streaming pipelines to capture clickstream event data from a Message Hub operator. The data is then written to a CSV file on Object Storage for later analysis. \n\nYou can do the following steps to access those CSV files for further analysis.\n\n#### Accessing CSV files on Object Storage\n1. Log in to [Bluemix](https://console.bluemix.net/) by using your DSX credentials.\n1. Navigate to the space where the Object Storage instance is located. This space is what you selected when you created the DSX project.\n1. Open the Object Storage instance.\n\n#### Accessing CSV files on Object Storage manually\n1. Open the **Manage** tab, and then select the container that you specified when you created the data collection pipeline. \n1. Select a CSV file. In the \"Select Action\"\" list, select \"Download File\" to view it.\n\n#### Accessing CSV files on Object Storage programatically\n1. Open the **Service credentials** tab. Select a Key Name, and then click **View credentials**. \n1. Copy the credentials and provide this information whenever you want to load data files programatically, such as in [Notebook 3b: Static clickstream analysis](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-3b.ipynb).\n", "cell_type": "markdown"}, {"metadata": {}, "source": "\n***\n\n### Authors\n\nGlynn Bird is a Developer Advocate for Watson Data Platform at IBM. \n\nRaj Singh is a Developer Advocate for Watson Data Platform at IBM.\n\n***\nCopyright \u00a9 IBM Corp. 2017. This notebook and its source code are released under the terms of the MIT License.", "cell_type": "markdown"}], "nbformat": 4}