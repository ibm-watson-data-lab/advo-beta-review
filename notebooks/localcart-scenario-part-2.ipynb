{"metadata": {"kernelspec": {"name": "python2-spark20", "display_name": "Python 2 with Spark 2.0", "language": "python"}, "language_info": {"version": "2.7.11", "name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}, "file_extension": ".py", "nbconvert_exporter": "python"}}, "cells": [{"metadata": {}, "cell_type": "markdown", "source": "# LocalCart scenario part 2: Creating streams flows\n\n\n## Introduction \n\nA web or mobile app will trigger events as a user navigates a web site. These clickstream events indicate when a customer logs in, adds something to a basket, completes an order, and logs out. The events are placed into configured Message Hub (Apache Kafka) that provides a scalable way to buffer the data before it is saved, analysed, and rendered. Using the instructions in [Notebook #1 - Creating a Kafka Producer of ClickStream events](https://github.com/wdp-beta/get-started/blob/beta_2/notebooks/localcart-scenario-part-1.ipynb) we generate clickstream events for LocalCart and send them to Message Hub to show how data can be collected offline and streamed to the cloud later. A Java app continuously feeds a simulated stream of events to Message Hub. \n\nThis notebook is divided into two parts, describing how to use streams flows to perform streaming data analysis and save data for static analysis.\n\n\n<img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/nb2_flow.png\"></img>\n\n\n### Streaming data analysis\n\n[Example 1: Capturing clickstream events for real-time analysis](#intro_a). You can use streaming pipelines to performs event-based aggregation operations (calculate the number of currently open baskets and value of those baskets, ...) on the fly and store the results into a Redis database. The aggregated data can easily be visualized in real-time using web applications that monitor this database, as described in [Notebook#4:Visualize streaming data in a real-time dashboard](https://github.com/wdp-beta/get-started/blob/beta_2/notebooks/localcart-scenario-part-4.ipynb). \n\n\n### Static data analysis\n\n[Example 2: Capturing clickstream events for static analysis](#intro_b). You can also use streaming pipelines to store clickstream events (as-is or in modified form) in flat files, which can be processed offline - either by batch processes or interactively, as outlined in [Notebook#3b: Analyze static clickstreams](https://github.com/wdp-beta/get-started/blob/beta_2/notebooks/localcart-scenario-part-3b.ipynb).\n\n\nThis notebook runs on Python 2 with Spark 2.0."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"intro_a\"></a>\n\n***\n# Example 1: Capturing clickstream events for real-time analysis\n***\n\n\n<img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/streaming_analysis.png\"></img>\n\n\nIn this first example you will create a streams flow that ingests `login`, `add_to_basket` and `checkout` clickstream events, aggregates them according to our business needs and stores the aggregated data in a Redis database, which will be monitored by a real-time dashboard:\n\n<img src='https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/MARMARMAR_result.png'></img>\n\n## Example 1 table of contents\n\n* [E1.1 Redis setup](#redis)<br>\n* [E1.2 Create a streams flow](#create_p1) <br>\n* [E1.3 Process login clickstream events](#login) <br>\n* [E1.4 Process add_to_cart clickstream events](#addtocart) <br>\n* [E1.5 Process checkout clickstream events](#checkout) <br>\n* [E1.6 Run the flow](#run_1)<br>\n* [E1.7 Summary and next steps](#summary_1)<br>\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"redis\"></a>\n***\n\n## E1.1 Redis setup\n\nRedis is an in-memory database. It stores its data in RAM, making it a very fast way of storing and retrieving data. It provides a set of primitive data structures, but we only concern ourselves with [hashes](https://redis.io/commands#hash) for this exercise.\n\nA Redis hash is a data structure that allows several keys to be stored together. We are going to configure a Redis hash called `funnel` that contains the following output:\n\n- login_count - the number of people who logged into LocalCart\n- basket_count - the number of items added into a shopping cart\n- checkout_count - the number of purchases made\n- basket_total - the total price of items added into a shopping cart\n- checkout_total - the total price of items purchased\n\nThese are the outputs of the aggregation functions in our streaming pipeline. \n\n\n### E1.1.1 Collect your Redis connection information\n\n1. Open your <a target=\"_blank\" href=\"https://apsportal.ibm.com/settings/services?context=analytics\">IBM Cloud Data Services list</a>. A list of your provisioned services is displayed.\n1. Locate the pre-provisioned **Compose for Redis** service and click on the service instance name.\n1. Open the _Service Credentials_ tab and view the credentials.\n```\n{\n  \"db_type\": \"redis\",\n  \"maps\": [],\n  \"name\": \"b...b\",\n  \"uri_cli\": \"redis-cli -h **HOSTNAME** -p **PORT** -a **PASSWORD**\",\n  \"deployment_id\": \"5...2\",\n  \"uri\": \"redis://admin:**PASSWORD**@**HOSTNAME**:**PORT**\"\n}\n```\n\nNote your `**HOSTNAME**`, `**PORT**` and `**PASSWORD**` information.\n\n\n### E1.1.2 Verify your redis connectivity\nYou can verify your redis connectivity information in this notebook by installing the Python Redis library with the following command:"}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null, "source": "!pip install redis"}, {"metadata": {}, "cell_type": "markdown", "source": "We import the library and connect to Redis with the following command. Replace the credential placeholders with your credentials."}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null, "source": "import redis\n# TODO replace **HOSTNAME**, **PORT** and **PASSWORD** with your credentials\nr = redis.StrictRedis(host='**HOSTNAME**', port=**PORT**, db=0, password='**PASSWORD**')"}, {"metadata": {}, "cell_type": "markdown", "source": "We can then create a hash called `funnel` to store our real-time data to the database by using the `hset` function:"}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null, "source": "r.hset('funnel', 'basket_count', 554);\nr.hset('funnel', 'basket_total', 951);\nr.hset('funnel', 'checkout_count', 21);\nr.hset('funnel', 'checkout_total', 5400);\nr.hset('funnel', 'login_count', 100);"}, {"metadata": {}, "cell_type": "markdown", "source": "We can also use this connection to retrieve all the values from our `funnel` hash using `hgetall`:"}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null, "source": "r.hgetall('funnel')"}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** \nThe Redis connection above seems to freeze in this notebook after a minute or so. In this case, you will need to restart the notebook kernel to restore it.\n<BR>\nWe can now create streams flows that store aggregated data in Redis."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"create_p1\"></a>\n***\n\n## E1.2 Create a streams flow\n\nIn IBM Watson Data Platform, do these steps:\n\n1. Select a project that you want to contain the streams flows. Note that this project must be attached to Cloud Object Storage and not Object Storage (Swift).\n1. Click the **Assets** tab and scroll to the _Streams flows_ section. (If no section with the name is displayed the selected project is not attached to Cloud Object Storage.)\n1. Click **+ New streams flow**.\n1. In the _New Streams Flow_ window, \n  1. Enter name `aggregate_for_redis`\n  1. Select an existing Streaming Analytics service or create a new one (choosing the _Lite_ plan, which is free.) \n  1. Select **Manually**. (You will use the wizard in Example 2.)\n  1. Click **Create**.\n\nAn empty canvas is displayed, along with a list of _Source_, _Target_, _Processing and Analytics_ and _Alerts_ operators that you can choose from. Source operators load data and target operators store data.\n\n<a id=\"login\"></a>\n***\n\n## E1.3 Process login clickstream events\n\nFirst we need to collect `login` data from Message Hub and calculate the number of logins during a rolling one hour time window. The incoming `login` event payload has the following structure:\n```\n  {\n    \"customer_id\": \"13872\",\n    \"click_event_type\": \"login\",\n    \"total_price_of_basket\": \"0.0\",\n    \"total_number_of_items_in_basket\": \"0\",\n    \"total_number_of_distinct_items_in_basket\": \"0\",\n    \"event_time\": \"2017-07-11 20:10:52 UTC\"\n  }\n```\n\n\n### E1.3.1 Configure the source\n\n1. Drag a **MessageHub** source operator into the pipeline canvas.\n1. Configure the MessageHub operator:\n\t1. Add a connection to your Message Hub instance.\n\t1. Select the `login` topic.\n\t1. Click **Edit Schema** to specify the payload properties this operator will make available to operators that are connected to its output port. Since we only want to count the number of login events we only make the `customer_id` available.\n    1. Choose\n            - Attribute Name: `customer_id`\n            - Type: `Number` \n            - Path: `/customer_id` \n    1. Click **Save** and **Close**.         \n\n\nOur streams flow now has its first operator and looks like this: \n\n<img src='https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/M.png'></img>\n\n\n### E1.3.2 Set up aggregation functions\n\nStreaming data can be aggregated by applying functions such as sum, count, minimum, or maximum. The results of the aggregation can be done on the aggregation before it is written to the Redis database. Our aim is to calculate the number of people who logged into LocalCart for a sliding one-hour window.\n\nIn the streams flow canvas, do these steps:\n\n1. Drag an **Aggregation** operator from the _Processing and Analytics_ area, and then drop it on the canvas next to the Message Hub operator.\n2. Drag your mouse pointer from the output port of the Message Hub operator to the input port of the Aggregation operator to connect them.\n3. Click the **Aggregation** operator to open its _Properties_ pane. Set the following _Aggregation Window_ parameters:\n    - Type - `sliding`\n    - Time Units - `hour`\n    - Number of Time Units - `1`\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n4. In the **Functions** area of the _Aggregation Properties_ pane, define one aggregation:\n    - Aggregation 1: count the logins\n        - Output Field Name - `login_count`\n        - Function Type - `Count`\n        \n    Note: To identify how many different customers have logged in during the rolling 1 hour time window, we would use the `CountDistinct` function and apply it to `customer_id`.\n\nOur streams flow now has two connected operators: a source operator and an aggregation operator. Hover over the arrow to review the data flow between them.\n\n<img src='https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/op_op_io.png'></img>\n\n\n\n### E1.3.3 Configure the target\n\nNext, add a Redis target operator. In the streams flow canvas, do these steps:\n\n1. Drag a **Redis** operator from the _Target_ area, and then drop it on the canvas next to the Aggregation operator.\n1. Drag your mouse pointer from the output port of the Aggregation operator to the input port of the Redis operator to connect them.\n1. Click the **Redis** operator to open its Properties pane. \n    - Add a connection to your Redis instance.\n      - Type in the `**HOST**`, `**PORT**` and `**PASSWORD**` credentials of your Compose for Redis service.\n    - In the **Key Template** field, type in `funnel`. \n1. Save the streams flow. The setup for `login` event processing is complete.\n\n  <img src='https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/MAR.png'></img>    \n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "***"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"addtocart\"></a>\n## E1.4 Process add_to_cart clickstream events\n\nNext we need to collect `add_to_cart` event data from Message Hub and calculate the number of shopping baskets and their combined value during a rolling one hour time window. The incoming `add_to_cart` event payload has the following structure:\n\n```\n{\n    \"customer_id\": \"13859\",\n    \"click_event_type\": \"add_to_cart\",\n    \"product_name\": \"Oatmeal\",\n    \"product_category\": \"Food\",\n    \"product_price\": \"2.49\",\n    \"total_price_of_basket\": \"153.41\",\n    \"total_number_of_items_in_basket\": \"19\",\n    \"total_number_of_distinct_items_in_basket\": \"6\",\n    \"event_time\": \"2017-06-23 12:56:18 UTC\"\n}\n```\n\n### E1.4.1 Configure the source, aggregation function and target for add_to_cart events\n\n1. Drag another **Message Hub** source operator into the canvas.\n1. Configure the Message Hub operator by doing these steps in the Properties pane:\n\t1. Select the Message Hub connection you've created earlier.\n\t1. Select the `add_to_cart` topic.\n\t1. Click **Edit Schema** to make the customer id and cart value available to connected operators. \n    1. The message schema can be automatically detected if a producer has already generated messages for the selected topic. Click **Detect Schema** and **Show preview**.\n      > If no messsages are displayed and the schema is not populated verify that your producer is running.\n    1. Click **Hide Preview**.\n    1. Remove all attributes except `customer_id` and `total_price_of_basket`:\n      - Attribute Name: `customer_id`\n            - Type: `Number` \n            - JSON Path: `/customer_id` \n      - Attribute Name: `total_price_of_basket` \n            - Type: `Number` \n            - JSON Path: `/total_price_of_basket` \n    1. Click **Save** and **Close**.\n1. Drag an **Aggregation** operator from the **Processing and Analytics** area, and then drop it on the canvas next to the Message Hub operator.\n1. Drag your mouse pointer from the output port of the Message Hub operator to the input port of the Aggregation operator to connect them.\n1. Click the **Aggregation** operator to open its _Properties_ pane. Set the following _Aggregation Window_ parameters:\n    - Type - `sliding`\n    - Time Units - `hour`\n    - Number of Time Units - `1`\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n1. In the **Functions** area of the _Aggregation Properties_ pane, define two aggregations:\n    - Aggregation 1: count the baskets\n        - Output Field Name - `basket_count`\n        - Function Type - `Count`\n    - Aggregation 2: Sum up basket values\n        - Output Field Name - `basket_total`\n        - Function Type - `Sum`\n        - Apply Function to - `total_price_of_basket`\n        \n1. Copy the existing **Redis** operator that's already on the canvas and paste it next to the _Aggregation_ Operator. \n1. Drag your mouse pointer from the output port of the Aggregation operator to the input port of the Redis operator to connect them.\n\n Your pipeline is now configured to stream and aggregate `login` and `add_to_cart` events:\n    \n <img src='https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/MARMAR.png'></img>    \n\n1. Save your streams flow. No errors should be reported.\n\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"checkout\"></a>\n***\n\n## E1.5 Process checkout clickstream events\n\nFirst we need to create a stream that collects `checkout` event data from a Message Hub operator and calculates the number of checkouts and their combined value during a rolling one hour time window. The incoming `checkout` event payload has the following structure:\n\n```\n{\n    \"customer_id\": \"11828\",\n    \"click_event_type\": \"checkout\",\n    \"total_price_of_basket\": \"72.80000000000001\",\n    \"total_number_of_items_in_basket\": \"20\",\n    \"total_number_of_distinct_items_in_basket\": \"5\",\n    \"session_duration\": \"440\",\n    \"event_time\": \"2017-06-23 13:09:12 UTC\"\n}\n```\n\n### E1.5.1 Set up pipeline source, aggregation function and target for checkout events\n\n1. Drag another **Message Hub** source operator into the canvas.\n1. Configure the MessageHub operator by doing these steps in the Properties pane:\n\t1. Select the ClickStream Message Hub connection.\n\t1. Select the `checkout` topic.\n\t1. Click **Edit Schema** to specify the message attributes that will be consumed. Define the following attributes (by entering them manually or customizing the auto-detected schema):\n      - Attribute Name: `customer_id` \n            - Type: `Number` \n            - Path: `/customer_id` \n      - Attribute Name: `total_price_of_basket` \n            - Type: `Number` \n            - Path: `/total_price_of_basket` \n1. Drag an **Aggregation** operator from the _Processing and Analytics_ area, and then drop it on the canvas next to the Message Hub operator.\n1. Drag your mouse pointer from the output port of the Message Hub operator to the input port of the Aggregation operator to connect them.\n1. Click the **Aggregation** operator to open its _Properties_ pane. Set the following _Aggregation Window_ parameters:\n    - Type - `sliding`\n    - Time Units - `hour`\n    - Number of Time Units - `1`\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n1. In the **Functions** area of the _Aggregation Properties_ pane, define two aggregations:\n    - Aggregation 1: count checkouts\n        - Output Field Name - `checkout_count`\n        - Function Type - `Count`\n    - Aggregation 2: Sum basket values\n        - Output Field Name - `checkout_total`\n        - Function Type - `Sum`\n        - Apply Function to - `total_price_of_basket`\n        \n1. Copy the existing **Redis** operator that's already on the canvas and paste it next to the _Aggregation_ Operator. \n1. Drag your mouse pointer from the output port of the Aggregation operator to the input port of the Redis operator to connect them. The completed stream now looks as follows: <br>\n   <img src='https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/MARMARMAR.png'></img>    \n\n1. Save the stream flow. No errors should be reported.\n\n<a id=\"run_1\"></a>\n## E1.6 Run the stream flow\n\n1. Click **Run**. \n1. If the flow does not start verify your stream flow. If no events are flowing from Message Hub operators make sure that your producer (simulating user activity), which you've launched in notebook 1, is running. \n1. Click on any operator to display throughput information.\n\n<img src= \"https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/nb2_redis_streaming_status.png\"></img>\n\nCongratulations! You just created a flow that ingests clickstream data from Message Hub, aggregates data and stores it in Redis storage.\n\n\n<a id=\"summary_1\"></a>\n## E1.7 Summary and next steps\nIn this section, you consumed and aggregated clickstream events that were generated in [Notebook #1: Creating a Kafka Producer of ClickStream events](https://github.com/wdp-beta/get-started/blob/beta_2/notebooks/localcart-scenario-part-1.ipynb).\n\nYou can now skip to [Notebook#4:Visualize streaming data in a real-time dashboard](https://github.com/wdp-beta/get-started/blob/beta_2/notebooks/localcart-scenario-part-4.ipynb) to learn about how to visualize the aggregated data in real-time using a simple web application or continue to the next section to configure a pipeline for static analysis.\n\n<img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/nb2_dashboard.png\"></img>\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"intro_b\"></a>\n\n***\n# Example 2: Capturing clickstream events for static analysis\n***\n\n\nIn this second example you will create multiple pipelines that ingest all clickstream events and store them as-is in CSV files on Cloud Object Storage in preparation for static analysis, as illustrated in [Notebook#3b: Analyze static clickstreams](https://github.com/wdp-beta/get-started/blob/beta_2/notebooks/localcart-scenario-part-3b.ipynb).\n\n<img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/static_analysis.png\"></img>\n\n\n## Example 2 table of contents\n\n* [E2.1 Capture login clickstream events](#login_2) <br>\n* [E2.2 Capture browsing clickstream events](#browsing_2) <br>\n* [E2.3 Capture add_to_cart clickstream events](#atc_2) <br>\n* [E2.4 Capture checkout clickstream events](#checkout_2) <br>\n* [E2.5 Capture logout_with_purchase clickstream events](#lwp_2) <br>\n* [E2.6 Capture logout_without_purchase clickstream events](#lwop_2) <br>\n* [E2.7 Summary and next steps](#summary_2)<br>\n\n\n<a id=\"login_2\"></a>\n***\n\n## E2.1 Create a flow for login clickstream events\n\nIn IBM Watson Data Platform, do these steps:\n\n1. Select a project that you want to contain the flow.\n1. Click the **Assets** tab and scroll to the _Streams flows_ section.\n1. Click **+ New streams flow**.\n1. In the _New Streams Flow_ window, \n  1. Enter streams flow name `store_events_on_cos`.\n  1. Select **Wizard**. \n  1. Click **Create**.\n1. In the _Select Source_ tab click **Message Hub**.\n1. Under the Connection drop-down menu, select your Message Hub connection.\n1. Under the Topic drop-down menu, select **login** and click **Continue**.\n1. Wait for the Data Preview window to display the streaming data for the selected event. (If no data is displayed make sure your producer is running.)\n > You can customize the pre-defined schema (e.g. remove attributes or change data types) by clicking _Edit schema_. In this scenario we want to store the complete event payload, therefore no changes are required. \n1. Click **Continue**.\n1. In the Select Target window, click **Cloud Object Storage**.\n1. Under the Cloud Object Storage connection drop-down menu, select the instance that is used by the project.\n   <br>\n   > Take note of the Cloud Object Storage connection name. You will need this information in [Notebook 3b: Static clickstream analysis](https://github.com/wdp-beta/get-started/blob/beta_2/notebooks/localcart-scenario-part-3b.ipynb) when you load and analyze the clickstream events.\n1. Under _File path_, select an existing bucket and append file name **login_%TIME.csv**. Your file path should look similar to this: `/existing-bucket-name/login_%TIME.csv`. (**Note:** \"TIME\" is a reserved word that will be replaced with an actual timestamp when the file is written).\n   > Note: if you choose a file name other than `login_%TIME.csv` you must also modify notebook 3B and change the default file name in the data load cell.\n1. Under _Format_, select **csv**.\n1. Under _Delimiter_, select **Comma (,)**.\n1. Under _File Creation Policy_ choose **Time** and **60** seconds.\n   > Note: Always choose a creation policy that apropriately reflects your data throughput and persistence requirements.\n1. Click **Continue** and **Save**. \n1. Click **Run** to start the streams flow. If no errors are reported all login events are written to timestamped CSV files in Cloud Object Storage.\n      <img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/nb2_csv_login_monitoring.png\"></img>\n1. Click **Stop** to stop the streams flow.\n\n> You can backup a flow by downloading/exporting it.\n\nOpen the the streams flow editor again to add the processing logic for the other clickstream event types.\n\n<a id=\"browsing_2\"></a>\n***\n\n## E2.2 Capture browsing clickstream events\n\n1. Copy and paste the existing **Message Hub** operator.\n1. Configure the new operator properties as follows:\n 1. Rename the operator to LC_BRWS to make it easily distinguishable from other Message Hub operators.\n  > You can choose any name.\n 1. Keep the existing connection.\n 1. Select the **browsing** topic.\n 1. Edit the schema by detecting it. (You will be prompted to override the current definition.)\n1. Copy and paste the existing **Cloud Object Storage** operator.\n1. Connect the output port of the Message Hub operator with the input port of the Cloud Object Storage operator.\n1. Configure the new operator properties as follows:\n 1. If desired, rename the operator. \n 1. Keep the existing Cloud Object Storage connection.\n 1. Under _File path_, select an existing bucket and append file name **browsing_%TIME.csv**. Your file path should look similar to this: `/existing-bucket-name/browsing_%TIME.csv`. \n 1. Under _Format_, select **csv**.\n 1. Under _Delimiter_, select **Comma (,)**.\n 1. Under _File Creation Policy_ choose **Time** and **60** seconds.\n1. Save the flow. \n\n\n<a id=\"atc_2\"></a>\n***\n\n## E2.3 Capture add_to_cart clickstream events\n\n1. Copy and paste one of the existing **Message Hub** operators.\n1. Configure the new operator properties as follows:\n 1. Rename the operator to LC_ADD to make it easily distinguishable from other Message Hub operators.\n 1. Keep the existing connection.\n 1. Select the **add_to_cart** topic.\n 1. Edit the schema by detecting it. (You will be prompted to override the current definition.)\n1. Copy and paste one of the existing **Cloud Object Storage** operators.\n1. Connect the output port of the new Message Hub operator with the input port of the new Cloud Object Storage operator.\n1. Configure the new Cloud Object Storage operator properties as follows:\n 1. If desired, rename the operator. \n 1. Keep the existing Cloud Object Storage connection.\n 1. Under _File path_, select an existing bucket and append file name **addtocart_%TIME.csv**. Your file path should look similar to this: `/existing-bucket-name/addtocart_%TIME.csv`. \n 1. Under _Format_, select **csv**.\n 1. Under _Delimiter_, select **Comma (,)**.\n 1. Under _File Creation Policy_ choose **Time** and **60** seconds.\n1. Save the flow. \n\n\n<a id=\"checkout_2\"></a>\n***\n\n## E2.4 Capture checkout clickstream events\n\n1. Copy and paste one of the existing **Message Hub** operators.\n1. Configure the operator properties as follows:\n 1. Rename the operator to LC_CHKO to make it easily distinguishable from other Message Hub operators.\n 1. Keep the existing connection.\n 1. Select the **checkout** topic.\n 1. Edit the schema by detecting it. (You will be prompted to override the current definition.)\n1. Copy and paste one of the existing **Cloud Object Storage** operators.\n1. Connect the output port of the new Message Hub operator with the input port of the new Cloud Object Storage operator.\n1. Configure the new Cloud Object Storage operator properties as follows:\n 1. If desired, rename the operator. \n 1. Keep the existing Cloud Object Storage connection.\n 1. Under _File path_, select an existing bucket and append file name **checkout_%TIME.csv**. Your file path should look similar to this: `/existing-bucket-name/checkout_%TIME.csv`. \n 1. Under _Format_, select **csv**.\n 1. Under _Delimiter_, select **Comma (,)**.\n 1. Under _File Creation Policy_ choose **Time** and **60** seconds.\n1. Save the flow. \n\n\n<a id=\"lwp_2\"></a>\n***\n\n## E2.5 Capture logout_with_purchase clickstream events\n\n1. Copy and paste one of the existing **Message Hub** operators.\n1. Configure the operator properties as follows:\n 1. Rename the operator to LC_PRCH to make it easily distinguishable from other Message Hub operators.\n 1. Keep the existing connection.\n 1. Select the **logout_with_purchase** topic.\n 1. Edit the schema by detecting it. (You will be prompted to override the current definition.)\n1. Copy and paste one of the existing **Cloud Object Storage** operators.\n1. Connect the output port of the new Message Hub operator with the input port of the new Cloud Object Storage operator.\n1. Configure the new Cloud Object Storage operator properties as follows:\n 1. If desired, rename the operator. \n 1. Keep the existing Cloud Object Storage connection.\n 1. Under _File path_, select an existing bucket and append file name **logoutwithpurchase_%TIME.csv**. Your file path should look similar to this: `/existing-bucket-name/logoutwithpurchase_%TIME.csv`. \n 1. Under _Format_, select **csv**.\n 1. Under _Delimiter_, select **Comma (,)**.\n 1. Under _File Creation Policy_ choose **Time** and **60** seconds.\n1. Save the flow. \n\n<a id=\"lwop_2\"></a>\n***\n\n## E2.6 Capture logout_without_purchase clickstream events\n\n1. Copy and paste one of the existing **Message Hub** operators.\n1. Configure the operator properties as follows:\n 1. Rename the operator to LC_NOPRCH to make it easily distinguishable from other Message Hub operators.\n 1. Keep the existing connection.\n 1. Select the **logout_without_purchase** topic.\n 1. Edit the schema by detecting it. (You will be prompted to override the current definition.)\n1. Copy and paste one of the existing **Cloud Object Storage** operators.\n1. Connect the output port of the new Message Hub operator with the input port of the new Cloud Object Storage operator.\n1. Configure the new Cloud Object Storage operator properties as follows:\n 1. If desired, rename the operator. \n 1. Keep the existing Cloud Object Storage connection.\n 1. Under _File path_, select an existing bucket and append file name **logoutwithoutpurchase_%TIME.csv**. Your file path should look similar to this: `/existing-bucket-name/logoutwithoutpurchase_%TIME.csv`. \n 1. Under _Format_, select **csv**.\n 1. Under _Delimiter_, select **Comma (,)**.\n 1. Under _File Creation Policy_ choose **Time** and **60** seconds.\n1. Save the flow. \n\n  <img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/nb2_store_to_cos_stream.png\"></img>\n\n\n1. Click **Run**. Six flows should be running, capturing `add_to_cart`, `browsing`, `checkout`, `login`, `logout_with_purchase`, and `logout_without_purchase` events and storing them in CSV files on Cloud Object Storage:\n <img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/nb2_csv_flows_monitoring.png\"></img>\n\n\n<a id=\"summary_2\"></a>\n***\n\n## E2.7 Summary and next steps\n\nYou can now do one of the following:\n\n#### Accessing CSV files on Cloud Object Storage\n1. Open your <a target=\"_blank\" href=\"https://apsportal.ibm.com/settings/services?context=analytics\">IBM Cloud Data Services list</a>. A list of your provisioned services is displayed.\n1. Locate the pre-provisioned Cloud Object Storage service and click on the service instance name.\n\n#### Accessing CSV files on Cloud Object Storage manually\n1. Open the **Buckets and objects** tab, and then select the bucket that you specified when you configured the Cloud Object Storage target operators. \n1. Click on the download link next to a CSV file.\n\n#### Accessing CSV files on Cloud Object Storage programatically\n1. Open the **Service credentials** tab. Select a Key Name, and then click **View credentials**. \n1. Copy the credentials and provide this information whenever you want to load data files programatically, such as in [Notebook 3b: Static clickstream analysis](https://github.com/wdp-beta/get-started/blob/beta_2/notebooks/localcart-scenario-part-3b.ipynb).\n"}, {"metadata": {}, "cell_type": "markdown", "source": "\n***\n\n### Authors\n\nGlynn Bird is a Developer Advocate for Watson Data Platform at IBM. \n\nRaj Singh is a Developer Advocate for Watson Data Platform at IBM.\n\n***\nCopyright \u00a9 IBM Corp. 2017, 2018. This notebook and its source code are released under the terms of the MIT License."}], "nbformat_minor": 1, "nbformat": 4}
