{"metadata": {"language_info": {"name": "python", "pygments_lexer": "ipython2", "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 2}, "version": "2.7.11"}, "kernelspec": {"name": "python2-spark20", "language": "python", "display_name": "Python 2 with Spark 2.0"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"source": "# LocalCart scenario part 2: MessageHub to CSV streaming pipelines", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "<a id=\"intro\"></a>\n## Introduction\n\n\nA web or mobile app will trigger events as a user navigates a web site. These clickstream events indicate when a customer logs in, adds something to a basket, completes an order, and logs out. The events are placed into configured Message Hub (Apache Kafka) that provides a scalable way to buffer the data before it is saved, analysed, and rendered. \n\n[Notebook #1 - Creating a Kafka Producer of ClickStream events](https://apsportal.ibm.com/analytics/notebooks/c3aee820-01af-478f-bd0f-07d80866863f/view?projectid=81238e6c-a19b-4c5c-9e45-753dfe7b7de3&context=analytics) generates clickstream events for LocalCart and sends them to Message Hub to show how data can be collected offline and streamed to the cloud later. A [Java app](https://localcartkafkaproducer.mybluemix.net/LocalCartKafkaProducer/) continuously feeds a simulated stream of events to Message Hub. \n\nThis notebook creates streaming pipelines that ingest those clickstream events, and writes them to CSV format on Object Storage for later analysis.\n\nThese files can be concatenated and loaded into a Jupyter notebook. We can use [Pixiedust](https://github.com/ibm-cds-labs/pixiedust) to analyse the data. This type of analysis with Pixiedust is done in [Notebook #4: Visualize streaming data](https://apsportal.ibm.com/analytics/notebooks/d9fd6d78-d55f-4e83-b8ae-d465f7af256f/view?projectid=81238e6c-a19b-4c5c-9e45-753dfe7b7de3&context=analytics).\n\nThis notebook runs on Python 2 with Spark 2.0.", "metadata": {}, "cell_type": "markdown"}, {"source": "## Table of contents\n\n1. [Introduction](#intro)<br>\n2. [Scenario](#process)<br>\n3. [Collect data from Message Hub](#collect)<br>\n4. [Steps](#steps)<br>", "metadata": {}, "cell_type": "markdown"}, {"source": "<a id=\"process\"></a>\n## Scenario \n\nIn this notebook, our aim is to persist the incoming events as CSV files by using the streaming pipelines service. The following graphic shows LocalCart clickstream events that are generated and sent from the Message Hub service. \n<img src='https://github.com/ibm-watson-data-lab/advo-beta-producer/blob/master/graphics/NB2a_CSV_PIPELINE.png?raw=true'></img>\n", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "<a id=\"collect\"></a>\n## Collect data from Message Hub\n\nFirst we need to create a streaming pipeline that collects data from a Message Hub operator.\n\n\n***\n\n<a id=\"steps\"></a>\n### Steps\n\nIn IBM Data Science Experience, do these steps:\n\n1. Select a project that you want to contain the streaming pipeline.\n1. Click the **Analytics Assets** tab\n1. In the Streaming Pipelines section, click **add streaming pipelines**.\n1. In the Create Streaming Pipeline window, click **Create with a Wizard**. \n1. In the Select Source window, click **MessageHub**.\n1. Under the Instance drop-down menu, select your MessageHub instance.\n1. Under the Topic drop-down menu, select **add_to_cart**. Click the Continue button.\n1. Wait for the Data Preview to load. Click the Continue button.\n1. In the Select Target window, click **Object Storage**.\n1. Under the Object Storage Instance drop-down menu, select your Object Storage instance.\n   <br>\n   > Take note of the  Object Storage instance name. You will need this information in notebook 3B (TODO) when you load and analyze the clickstream events.\n1. Under the Container drop-down menu, select the Object Storage container you want to write to. \n   <br>\n   > Take note of the  Object Storage container name. You will need this information in notebook 3B (TODO) when you load and analyze the clickstream events.\n1. Under File Name, type **add_to_cart-TIMESTAMP.csv** (note: **TIMESTAMP** is a reserved word which will be replaced with an actual timestamp when the file is written).\n1. Under Format, select **csv**.\n1. Under Delimiter, select **Comma (,)**.\n1. Type in a name for the pipeline, such as **addtocart2csv**, and then click **Save**.\n1. In the next window, click the **Run** icon.\n1. Repeat the steps above for each MessageHub topic: browsing, checkout, clickStream, login, logout_with_purchase, and logout_without_purchase", "metadata": {}, "cell_type": "markdown"}, {"source": "## Accessing CSV files on Object Storage\n1. Login to [Bluemix](https://console.bluemix.net/) using your DSX credentials.\n1. Navigate to the space where the Object Storage instance is located that you've selected when you created the DSX project.\n1. Open the Object Storage instance.\n\n### Accessing CSV files on Object Storage manually\n1. Open the _Manage_ tab and select the container you've specified when you created the data collection pipeline. \n1. Select a csv file and \"Select Action\" > \"Download File\" to view it.\n\n### Accessing CSV files on Object Storage programatically\n1. Open the _Service credentials_ tab and select _View credentials_.\n1. Copy the credentials and provide this information whenever you want to load data files programatically, such as in notebook 3B (TODO).\n", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "", "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}]}