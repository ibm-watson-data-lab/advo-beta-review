{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"collapsed": true}, "source": "# LocalCart scenario part 1: Creating a Kafka Producer of clickstream events"}, {"cell_type": "markdown", "metadata": {}, "source": "## Introduction\nThe Apache Kafka Producer is an IBM WebSphere Liberty Java EE application that generates a stream of clickstream events to simulate user behavior on a web site. For example, log in, browse, add to cart, purchase, and log out events are all simulated. \n\nThere are two ways to deploy the Kafka Producer:\n- Run the Kafka Producer directly from the notebook.\n- Deploy the Kafka Producer in Bluemix Liberty runtime environment. If you want to make changes to the Kafka Producer, the notebook guides you how to customize and rebuild the Kafka Producer code.\n\nThe figure below shows the flow from the Jupyter notebook or a Bluemix Liberty environment, to a Kafka Producer, to a Message Hub, and to a streaming pipeline in [Notebook #2: Creating a streaming pipeline](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-2.ipynb). \n\n<img src='https://github.com/notebookgraphics/advobeta/blob/master/NB1_FLOW_OVERVIEW_sm.gif?raw=true'></img>\n\nThe Kafka Producer code can be found at [advo-beta-producer](https://github.com/ibm-cds-labs/advo-beta-producer).\n\nThis notebook runs on Python 2 with Spark 2.0. "}, {"cell_type": "markdown", "metadata": {}, "source": "## Table of contents\n1. [Setup](#setup)<br>\n\n2. [Run the Kafka Producer directly from a Jupyter notebook](#run_direct)<br>\n    4.1 [Import the Kafka Producer jar](#run_direct_step_1)<br>\n    4.2 [Set the Message Hub credentials](#run_direct_step_2)<br>\n    4.3 [Start the DataStream](#run_direct_step_3)<br>\n    4.4 [Verify that events are flowing](#run_direct_step_4)<br>\n    \n3. [Deploy a Kafka Producer in a Bluemix Liberty runtime environment](#deploy)<br>\n\n4. [(Optional) Customize and rebuild the Kafka Producer code](#customize)<br>\n\n5. [Summary and next steps](#summary)<br>\n    "}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"setup\"></a>\n## Setup\n\n### Collect Message Hub connection information\n\n1. Open your <a target=\"_blank\" href=\"https://apsportal.ibm.com/settings/services?context=analytics\">Bluemix Data Services list</a>. A list of your provisioned services is displayed.\n1. Locate the pre-provisioned **Message Hub** service and click on the service instance name.\n1. Open the _Service Credentials_ tab and view the credentials.\n1. Note the **user**, **password**, and **api_key** credentials.\n1. Skip the next setup step if you are planning to run the producer from the notebook.\n\n\n### (Optional) Install the deployment prerequisites \nIf you are planning to manually deploy the producer to Bluemix\n1. [Download the CloudFoundry CLI](https://github.com/cloudfoundry/cli#downloads) on your local machine. This app is needed to deploy the Kafka Producer.\n2. [Install git](https://git-scm.com/downloads) CLI.\n3. (Optional) Download the [Eclipse IDE](https://developer.ibm.com/wasdev/downloads/#asset/tools-IBM_Eclipse_Tools_for_Bluemix) and install the Liberty development tooling. "}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"run_direct\"></a>\n## Run the Kafka Producer directly from a notebook\nIf you want to get started quickly and generate events, you can simply run the following cells directly from your notebook. \n\nThe drawback is that the event generation depends on the notebook session being kept alive. When the session stops, event generation also stops.\n\nThe code below uses the PixieDust PackageManager to load the Kafka Producer code from a jar file. It then uses the Scala bridge to directly invoke the DataStream class and start producing events.\n\n<a id=\"run_direct_step_1\"></a>\n### 1. Import the Kafka Producer jar by using the PixieDust PackageManager"}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": "import pixiedust\npixiedust.installPackage(\"https://github.com/ibm-watson-data-lab/advo-beta-producer/raw/master/dist/LocalCartKafkaProducer-1.0-SNAPSHOT-uber.jar\")", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"run_direct_step_2\"></a>\n### 2. Set the Message Hub credentials\nReplace the `**USER**`, `**PASSWORD**`, and `**API_KEY**` placeholders in the cell below with the credentials from your setup."}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": "%%scala\nSystem.setProperty(\"KAFKA_USER_NAME\",\"**USER**\")\nSystem.setProperty(\"KAFKA_PASSWORD\",\"**PASSWORD**\")\nSystem.setProperty(\"KAFKA_API_KEY\",\"**API_KEY**\")\nSystem.setProperty(\"USE_JAAS\", \"true\")", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"run_direct_step_3\"></a>\n### 3. Start the data stream"}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": "%%scala\nimport com.ibm.localcart.DataStream\nprintln(DataStream.getInstance());", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"run_direct_step_4\"></a>\n### 4. Verify that events are flowing by looking at the stats\n> You can run this cell multiple times to make sure that events continue to be generated."}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": "%%scala\nimport com.ibm.localcart.DataStream\nprintln(DataStream.getInstance().getStats)", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"deploy\"></a>\n## Deploy a Kafka Producer in a Bluemix Liberty runtime environment\n\n1. Clone the repository by running this command: `git clone https://github.com/ibm-watson-data-lab/advo-beta-producer.git`\n2. [Connect to Bluemix by using the CloudFoundry CLI](https://console.bluemix.net/docs/runtimes/liberty/getting-started.html#getting-started-with-liberty-on-bluemix). To select the space and org that you created in the Setup section above, see step 4 in [Getting started with Liberty in Bluemix](https://console.bluemix.net/docs/runtimes/liberty/getting-started.html#getting-started-with-liberty-on-bluemix). Do these steps at a command line: \n    - `cf api <API_endpoint>` - where `<API_endpoint>` is the API region that you selected in the Setup section. For example, `cf api https://api.ng.bluemix.net`\n    - `cf login` - to log in to your Bluemix account. For example, `cf login my_login@aaa.com`\n    - From the list of organizations, select the organization. The \"Targeted organization\" and \"Targeted space\" are what you need.\n\n3. Customize the Cloud Foundry manifest by doing these steps:\n  - Open the file [`manifest.yml`](https://github.com/ibm-watson-data-lab/advo-beta-producer/blob/master/manifest.yml) in your favorite text editor. \n  - In the `env:` section, assign values for the following three variables (from the credentials in the [Setup](#setup) section, step 3): \n    - `KAFKA_USER_NAME` : MessageHub `user` name from service credentials  \n    - `KAFKA_PASSWORD` : MessageHub `password` from service credentials  \n    - `KAFKA_API_KEY` : MessageHub `api_key` from service credentials  \n4. Deploy Bluemix by using the following command: `cf push -f manifest.yml -p ./dist/kafkaProducerServer.zip`\n5. In a web browser, navigate to the application URL, for example `https://local-cart-producer-....mybluemix.net/LocalCartKafkaProducer/`. Note that the URL will contain random words in the host name, such as `local-cart-producer-unalphabetized-misshipment`. To determine your application's URL run `cf apps` and inspect the listed `urls` value.\n   <br>\n   The app should show you the number of events that are being generated. You can configure the generated rate of events to be between 0 and 20 events per second. **Note:** Choosing 0 events will pause the Kafka Producer."}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"customize\"></a>\n## (Optional) Customize and rebuild the Kafka Producer code \n\n1. If Eclipse is not installed, install [Eclipse Neon for Java EE Developers (4.6.1)](http://www.eclipse.org/downloads/eclipse-packages/).\n\n2. Click and hold this <a href=\"http://marketplace.eclipse.org/marketplace-client-intro?mpc_install=1774120\" target=\"_blank\"><img src=\"https://console.bluemix.net/docs/api/content/starters/images/installbutton.png?lang=en\" style=\"display:inline-block\" alt=\"Drag and drop into a running Eclipse Neon workspace to install IBM Eclipse Tools for Bluemix\"></a> button to drag and drop it into the Eclipse toolbar. Follow the prompts to install IBM Eclipse Tools for Bluemix.\n\n3. Import the advo-beta-producer project that you cloned in step 1 of the [Deploy a Kafka Producer](#deploy) section.\n\n4. Become familiar with the Producer code:  \n\n    a. **com.ibm.localcart.DataStream:** Singleton class responsible for generating the Kafka events. It spawns a thread that executes every second and generate new events from the master [JSON file](https://github.com/ibm-watson-data-lab/advo-beta/raw/master/data/dataStream.json) on github. The number of events generated each second is configurable from the UI. The default is 2.\n    \n    b. **com.ibm.localcart.MessageHubConfig:** Singleton class responsible for configuring the Kafka Producer.  \n    \n    c. **com.ibm.localcart.StatsAPI:** Jax-rs application class that provide rest end-points like *getStats* and *getConfig* that are used by the UI.\n    \n    d. **/src/main/webapp/index.html:** Main UI that provide statistics for each topic generated. It also allows you to change the number of events that are being generated from 0 - 20. To pause the Producer, set the slider to 0.  \n    \n    e. **/src/main/wlp/server.xml:** Server configuration. It contains server deployment information. Take a minute to look at the jaasLoginModule section that configures jaas security for Message Hub. It uses the KAFKA_PASSWORD and KAFKA_USER_NAME variables that can be passed in server.env\n    \n    f. Optional: As an exercise, change how events are generated to use a different URL than the github URL that is currently used. You could also use a more realistic scenario where the events are read from a data source such as a relational database.  \n    \n    g. Because we need the custom jaasLoginModule secton in server.xml, we need to create a new zip file in order to deploy on Bluemix. For instructions about how to generate the zip file, see [Custom Liberty server.xml configurations in IBM Bluemix](https://www.ibm.com/blogs/bluemix/2015/01/modify-liberty-server-xml-configurations-ibm-bluemix/).  \n    \n    h. Deploy the zip from a command line terminal by using the following command: `cf push LocalCartKafkaProducer -p ./kafkaProducerServer.zip`  "}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"summary\"></a>\n## Summary and next steps\nIn this notebook you set up a stream of data that includes clickstream events (log in, browse, add to cart, logout without purchase, logout with purchase, and checkout). Use this data in [notebook 2 - Creating a streaming pipeline](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-2.ipynb).\n\n### Author\nDavid Taieb is a Distinguished Engineer for Watson Data Platform and a Developer Advocate at IBM. \n\n***\n\nCopyright \u00a9 IBM Corp. 2017. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"display_name": "Python 2 with Spark 2.0", "language": "python", "name": "python2-spark20"}, "language_info": {"version": "2.7.11", "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}, "file_extension": ".py", "name": "python"}}, "nbformat": 4}