{"metadata": {"language_info": {"codemirror_mode": {"version": 2, "name": "ipython"}, "nbconvert_exporter": "python", "file_extension": ".py", "version": "2.7.11", "mimetype": "text/x-python", "pygments_lexer": "ipython2", "name": "python"}, "kernelspec": {"display_name": "Python 2 with Spark 2.0", "language": "python", "name": "python2-spark20"}}, "nbformat": 4, "cells": [{"metadata": {}, "cell_type": "markdown", "source": "# LocalCart scenario part 3b: Analyze static clickstreams\n\nIn this notebook, you'll analyze clickstream data to understand how users interact with your online store. \n\n<img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/nb3b_static_analysis.png\"></img>\n\nA clickstream represents the sequential events that users triggered while they were interacting with the online store. These events include, but are not limited to: \n* view an item\n* add an item to cart\n* checkout a cart\n* complete the checkout process\n* abandon the checkout process\n\nThis notebook runs on Python 2 with Spark 2.0.\n\nBefore you run this notebook, complete the setup tasks https://datascience.ibm.com/docs/content/getting-started/WDP_Beta_Scenario.html and run these notebooks:\n\n1. LocalCart scenario part 1: [Generating a Kafka producer (JSON) into MessageHub](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-1.ipynb)\n1. LocalCart scenario part 2: [Building the streaming pipeline](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-2.ipynb)\n1. LocalCart scenario part 3: [Analyze customer demographics and sales data](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-3.ipynb)"}, {"metadata": {}, "cell_type": "markdown", "source": "## Table of contents\n[Setup](#Setup)<br>\n[Load static clickstream data](#Load-static-clickstream-data)<br>\n[Re-establish the original clickstream event sequence](#Re-establish-the-original-clickstream-event-sequence)<br>\n[Create customer clickstream histories](#Create-customer-clickstream-histories)<br>\n[View event statistics](#View-event-statistics)<br>\n[View event streams](#View-event-streams)<br>\n[Summary and next steps](#Summary-and-next-steps)"}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "## Setup \n\nImport the libraries that you'll use in this notebook:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "import pixiedust\nimport pyspark.sql.functions as func\nimport pyspark.sql.types as types\nimport re\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom pyspark.sql import SparkSession"}, {"metadata": {}, "cell_type": "markdown", "source": "## Load static clickstream data\nYou'll access the clickstream data files in your Object Storage service and then consolidate the data from the many clickstream files into DataFrames by event type. \n\nYou'll create DataFrames for these types of events:\n- Login \n- Browsing\n- Checkout\n- Add an item to a cart\n- Logout after purchasing\n- Logout without purchasing"}, {"metadata": {}, "cell_type": "markdown", "source": "### Configure Object Storage credentials\n\nThe clickstream files are stored in the Object Storage service that's associated with your DSX account. To access your object storage programmatically, you need to copy in your credentials, which you can see in your Object Storage service details in Bluemix.\n\n1. Open your [Bluemix Data Services list](https://apsportal.ibm.com/settings/services?context=analytics) in a new browser window. (You can also navigate to this list by clicking the avatar icon on the upper right hand side and selecting _Settings_ and _Services_.\n1. Click on your Object Storage service.\n1. Click **Service Credentials** and then **View credentials**.\n1. Copy the credentials text between the curly braces and paste it over the text between the braces in the next cell.\n1. Replace `<OBJECT_STORAGE_CONTAINER_NAME>` with the name of your object storage container that you used for your streaming pipelines. Most likely, it's the same container that you're using in this project. You can see the container name for your project on the project **Settings** page, in the **Storage** section.\n1. Run the cell."}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# @hidden_cell\n# TODO replace with your Object Storage credentials from Bluemix\nOS_credentials = {\n  \"auth_url\": \"https://identity.open.softlayer.com\",\n  \"projectId\": \"<...>\",\n  \"region\": \"<...>\",\n  \"userId\": \"<...>\",\n  \"password\": \"<...>\"\n}\n\n# TODO: replace with your Object Storage container name\ncontainer = '<OBJECT_STORAGE_CONTAINER_NAME>'"}, {"metadata": {}, "cell_type": "markdown", "source": "Define the following function to access your object storage:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# This function is used to set up the access of Spark to your Object Storage. It references the variables you set up for your credentials.\ndef set_hadoop_config_with_credentials(credentials, name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', '{}/v3/auth/tokens'.format(credentials['auth_url']))\n    hconf.set(prefix + '.tenant', credentials['projectId']) \n    hconf.set(prefix + '.username', credentials['userId'])\n    hconf.set(prefix + '.password', credentials['password'])\n    hconf.set(prefix + '.region', credentials['region'])\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.setBoolean(prefix + '.public', False)\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints') # optional\n\nname = 'keystone'\nset_hadoop_config_with_credentials(OS_credentials, name)\n\nspark = SparkSession.builder.getOrCreate()"}, {"metadata": {}, "cell_type": "markdown", "source": "### Load login clickstream events \nConsolidate login data from multiple clickstream files into a DataFrame named `login_df`:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# load login events\nfilename_pattern = 'login_*.csv'\n\npath = 'swift://{}.{}/{}'.format(container, name, filename_pattern)\n\nlogin_schema = types.StructType([\n                                 types.StructField('customer_id', types.StringType(), True),\n                                 types.StructField('click_event_type', types.StringType(), True),\n                                 types.StructField('total_price_of_basket', types.DecimalType(10,2), True),\n                                 types.StructField('total_number_of_items_in_basket', types.IntegerType(), True),\n                                 types.StructField('total_number_of_distinct_items_in_basket', types.IntegerType(), True),\n                                 types.StructField('click_event_time', types.TimestampType(), True),         \n                                ])\n\nprint 'Loading clickstream data for login events from {} ...'.format(path)\n\nlogin_df = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'false')\\\n  .option('timestampFormat', 'yyyy-MM-dd HH:mm:ss z')\\\n  .load(path, schema = login_schema)\n\nif login_df:\n    print 'Data structure: {}'.format(login_df)\n    print 'Event count: {:,}'.format(login_df.count())\n    print 'Example event: {}'.format(login_df.head())\nelse:\n    print 'Fatal error loading login events.'"}, {"metadata": {}, "cell_type": "markdown", "source": "### Load browsing clickstream events \nConsolidate browsing data from multiple clickstream files into a DataFrame named `browsing_df`:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# load browsing events\nfilename_pattern = 'browsing_*.csv'\n\npath = 'swift://{}.{}/{}'.format(container, name, filename_pattern)\n\nbrowsing_schema = types.StructType([\n                                    types.StructField('customer_id', types.StringType(), True),\n                                    types.StructField('click_event_type', types.StringType(), True),\n                                    types.StructField('total_price_of_basket', types.DecimalType(10,2), True),\n                                    types.StructField('total_number_of_items_in_basket', types.IntegerType(), True),\n                                    types.StructField('total_number_of_distinct_items_in_basket', types.IntegerType(), True),\n                                    types.StructField('click_event_time', types.TimestampType(), True),            \n                                   ])\n\nprint 'Loading clickstream data for browsing events from {} ...'.format(path)\n\nbrowsing_df = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'false')\\\n  .option('timestampFormat', 'yyyy-MM-dd HH:mm:ss z')\\\n  .load(path, schema = browsing_schema)\n\nif browsing_df:\n    print 'Data structure: {}'.format(browsing_df)\n    print 'Event count: {:,}'.format(browsing_df.count())\n    print 'Example event: {}'.format(browsing_df.head())\nelse:\n    print 'Fatal error loading browsing events.'"}, {"metadata": {}, "cell_type": "markdown", "source": "### Load checkout clickstream events \nConsolidate checkout data from multiple clickstream files into a DataFrame named `checkout_df`:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# load checkout events\nfilename_pattern = 'checkout_*.csv'\n\npath = 'swift://{}.{}/{}'.format(container, name, filename_pattern)\n\ncheckout_schema = types.StructType([\n                                    types.StructField('customer_id', types.StringType(), True),\n                                    types.StructField('click_event_type', types.StringType(), True),\n                                    types.StructField('total_price_of_basket', types.DecimalType(10,2), True),\n                                    types.StructField('total_number_of_items_in_basket', types.IntegerType(), True),\n                                    types.StructField('total_number_of_distinct_items_in_basket', types.IntegerType(), True),\n                                    types.StructField('session_duration', types.IntegerType(), True),\n                                    types.StructField('click_event_time', types.TimestampType(), True),            \n                                   ])\n\nprint 'Loading clickstream data for checkout events from {} ...'.format(path)\n\ncheckout_df = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'false')\\\n  .option('timestampFormat', 'yyyy-MM-dd HH:mm:ss z')\\\n  .load(path, schema = checkout_schema)\n\nif checkout_df:\n    print 'Data structure: {}'.format(checkout_df)\n    print 'Event count: {:,}'.format(checkout_df.count())\n    print 'Example event: {}'.format(checkout_df.head())\nelse:\n    print 'Fatal error loading checkout events.'"}, {"metadata": {}, "cell_type": "markdown", "source": "### Load add_to_cart clickstream events \nConsolidate data for adding items to a cart from multiple clickstream files into a DataFrame named `add_to_cart_df`:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# load add_to_cart events\nfilename_pattern = 'addtocart_*.csv'\n\npath = 'swift://{}.{}/{}'.format(container, name, filename_pattern)\n\nadd_to_cart_schema = types.StructType([\n                                       types.StructField('customer_id', types.StringType(), True),\n                                       types.StructField('click_event_type', types.StringType(), True),\n                                       types.StructField('product_name', types.StringType(), True),\n                                       types.StructField('product_category', types.StringType(), True),\n                                       types.StructField('product_price', types.DecimalType(10,2), True),\n                                       types.StructField('total_price_of_basket', types.DecimalType(10,2), True),\n                                       types.StructField('total_number_of_items_in_basket', types.IntegerType(), True),\n                                       types.StructField('total_number_of_distinct_items_in_basket', types.IntegerType(), True),\n                                       types.StructField('click_event_time', types.TimestampType(), True),            \n                                      ])\n\nprint 'Loading clickstream data for add_to_cart events from {} ...'.format(path)\n\nadd_to_cart_df = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'false')\\\n  .option('timestampFormat', 'yyyy-MM-dd HH:mm:ss z')\\\n  .load(path, schema = add_to_cart_schema)\n\nif add_to_cart_df:\n    print 'Data structure: {}'.format(add_to_cart_df)\n    print 'Event count: {:,}'.format(add_to_cart_df.count())\n    print 'Example event: {}'.format(add_to_cart_df.head())\nelse:\n    print 'Fatal error loading add_to_cart events.'"}, {"metadata": {}, "cell_type": "markdown", "source": "### Load logout_with_purchase clickstream events \nConsolidate data for logging out after purchasing from multiple clickstream files into a DataFrame named `logout_with_purchase_df`:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# load logout_with_purchase events\nfilename_pattern = 'logoutwithpurchase_*.csv'\n\npath = 'swift://{}.{}/{}'.format(container, name, filename_pattern)\n\nlogout_with_purchase_schema = types.StructType([\n                                                types.StructField('customer_id', types.StringType(), True),\n                                                types.StructField('click_event_type', types.StringType(), True),\n                                                types.StructField('total_price_of_basket', types.DecimalType(10,2), True),\n                                                types.StructField('total_number_of_items_in_basket', types.IntegerType(), True),\n                                                types.StructField('total_number_of_distinct_items_in_basket', types.IntegerType(), True),\n                                                types.StructField('session_duration', types.IntegerType(), True),                    \n                                                types.StructField('click_event_time', types.TimestampType(), True),            \n                                               ])\n\nprint 'Loading clickstream data for logout_with_purchase events from {} ...'.format(path)\n\nlogout_with_purchase_df = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'false')\\\n  .option('timestampFormat', 'yyyy-MM-dd HH:mm:ss z')\\\n  .load(path, schema = logout_with_purchase_schema)\n\nif logout_with_purchase_df:\n    print 'Data structure: {}'.format(logout_with_purchase_df)\n    print 'Event count: {:,}'.format(logout_with_purchase_df.count())\n    print 'Example event: {}'.format(logout_with_purchase_df.head())\nelse:\n    print 'Fatal error loading logout_with_purchase events.'"}, {"metadata": {}, "cell_type": "markdown", "source": "### Load logout_without_purchase clickstream events \nConsolidate data for logging out without purchasing from multiple clickstream files into a DataFrame named `logout_without_purchase_df`:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# load logout_without_purchase events\nfilename_pattern = 'logoutwithoutpurchase_*.csv'\n\npath = 'swift://{}.{}/{}'.format(container, name, filename_pattern)\n\nlogout_without_purchase_schema = types.StructType([\n                                                   types.StructField('customer_id', types.StringType(), True),\n                                                   types.StructField('click_event_type', types.StringType(), True),\n                                                   types.StructField('total_price_of_basket', types.DecimalType(10,2), True),\n                                                   types.StructField('total_number_of_items_in_basket', types.IntegerType(), True),\n                                                   types.StructField('total_number_of_distinct_items_in_basket', types.IntegerType(), True),\n                                                   types.StructField('session_duration', types.IntegerType(), True),                    \n                                                   types.StructField('click_event_time', types.TimestampType(), True),            \n                                                  ])\n\nprint 'Loading clickstream data for logout_without_purchase events from {} ...'.format(path)\n\nlogout_without_purchase_df = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'false')\\\n  .option('timestampFormat', 'yyyy-MM-dd HH:mm:ss z')\\\n  .load(path, schema = logout_without_purchase_schema)\n\nif logout_without_purchase_df:\n    print 'Data structure: {}'.format(logout_without_purchase_df)\n    print 'Event count: {:,}'.format(logout_without_purchase_df.count())\n    print 'Example event: {}'.format(logout_without_purchase_df.head())\nelse:\n    print 'Fatal error loading logout_without_purchase events.'"}, {"metadata": {}, "cell_type": "markdown", "source": "## Re-establish the original clickstream event sequence\n\nNow create a DataFrame that consolidates all the clickstream event data in the right order:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# Union click events\nevents_df = (login_df.select(\"click_event_time\", \"customer_id\", \"click_event_type\").\n                    union(browsing_df.select(\"click_event_time\", \"customer_id\", \"click_event_type\")).\n                    union(add_to_cart_df.select(\"click_event_time\", \"customer_id\", \"click_event_type\")).\n                    union(checkout_df.select(\"click_event_time\", \"customer_id\", \"click_event_type\")).                             \n                    union(logout_with_purchase_df.select(\"click_event_time\", \"customer_id\", \"click_event_type\")).  \n                    union(logout_without_purchase_df.select(\"click_event_time\", \"customer_id\", \"click_event_type\")))\n\nif events_df:\n    print 'Data structure: {}'.format(events_df)\n    print 'Event count: {:,}'.format(events_df.count())\nelse:\n    print 'Fatal error unioning clickstream events.'"}, {"metadata": {}, "cell_type": "markdown", "source": "Print the count of each type of event:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "events_df.groupBy(\"click_event_type\").count().orderBy('count', ascending = False).collect()"}, {"metadata": {}, "cell_type": "markdown", "source": "## Create customer clickstream histories \nFirst create single character abbreviations for each type of event. Then create a DataFrame with event histories for each customer."}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "event_type_dict = {\n    'login': 'l',\n    'browsing': 'b',\n    'add_to_cart': 'a',\n    'checkout': 'c',\n    'logout_with_purchase': 'p',\n    'logout_without_purchase': 'n'\n}\n\n# create clickstream history for each customer: Row(customer_id=u'13602', events=u'bbbbblbbbbblaaacp', CP_count=1, CN_count=0)\n# each character in events represents an event from the map above; 'U' indicates an unknown event\ncustomer_cs = {}\nfor row in events_df.orderBy('click_event_time').collect():\n    if customer_cs.get(row.customer_id):\n        customer_cs[row.customer_id] += event_type_dict.get(row.click_event_type,'U')\n    else:\n        customer_cs[row.customer_id] = event_type_dict.get(row.click_event_type,'U')\n\n# create dataframe\nevents_history_df = sc.parallelize([(k,)+(v,) for k,v in customer_cs.items()]).toDF(['customer_id','events'])\n\ndef matchPattern(col, pattern):\n    return len(re.findall(pattern,col))\n\nmatchPatternUDF = func.udf(lambda c,p: matchPattern(c,p), types.IntegerType())\n# count number of product views\nevents_history_df = events_history_df.withColumn(\"B_count\", matchPatternUDF(events_history_df['events'],func.lit('b')))\n# count number of add-to-cart\nevents_history_df = events_history_df.withColumn(\"BA_count\", matchPatternUDF(events_history_df['events'],func.lit('ba')))\n# count number of completed purchases (checkout -> logout_with_purchase)\nevents_history_df = events_history_df.withColumn(\"CP_count\", matchPatternUDF(events_history_df['events'],func.lit('cp')))\n# count number of aborted checkouts (checkout -> logout_without_purchase)\nevents_history_df = events_history_df.withColumn(\"CN_count\", matchPatternUDF(events_history_df['events'],func.lit('cn')))"}, {"metadata": {}, "cell_type": "markdown", "source": "## View event statistics\nLook at the summary statistics about the number of customers and each type of event."}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# Number of customers\nprint 'Customer count: {:,}'.format(events_history_df.count())\n# Number of product views\nprint 'Product views: {:,}'.format(events_history_df.agg({'B_count': 'sum'}).collect()[0][0])\n# Number of add-to-cart events\nprint 'Cart additions: {:,}'.format(events_history_df.agg({'BA_count': 'sum'}).collect()[0][0])\n# Number of customers that have completed at least one purchase\nprint 'Customers with purchase: {:,}'.format(events_history_df.filter('CP_count > 0').count())\n# Number of customers that have completed more than one purchase\nprint 'Customers with multiple purchases: {:,}'.format(events_history_df.filter('CP_count > 1').count())\n# Number of abandoned checkouts (checkout was never completed)\nprint 'Abandoned carts: {:,}'.format(events_history_df.agg({'CN_count': 'sum'}).collect()[0][0])"}, {"metadata": {}, "cell_type": "markdown", "source": "## View event streams\n\nNow take a look at some customer event streams. \n\nRemember the event type abbreviations:\n- l = login\n- b = browsing\n- a = add to cart\n- c = checkout\n- p = logout after purchasing\n- n = logout without purchasing\n- U = unknown event"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# sample event streams for 10 customers\nevents_history_df.head(10) "}, {"metadata": {}, "cell_type": "markdown", "source": "## Summary and next steps\nYou've learned how to get the data from multiple event streams into a format that you can use to analyze customer behavior.\n\nNext, learn how to create a dashboard for streaming data in [LocalCart scenario part 4: Visualize streaming data in a real-time dashboard](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-4.ipynb) notebook.\n\n\n## Author\n\nPatrick Titzler is a customer advocate for Watson Data Platform at IBM.\n<hr>\nCopyright &copy; IBM Corp. 2017. This notebook and its source code are released under the terms of the MIT License."}], "nbformat_minor": 1}