{"nbformat": 4, "cells": [{"source": "# LocalCart scenario part 6a: Creating an aggregation streams flow\n\n\n## Introduction \n\nThis notebook is divided into two parts. The first part (this notebook), describes how to import a streams flow that performs data enrichment, filtering and agregation. The second part implements a dashboard that visualizes the aggregated data.\n\nIn [Notebook #1 - Creating a Kafka Producer of ClickStream events](https://github.com/wdp-beta/get-started/blob/beta_2/notebooks/localcart-scenario-part-1.ipynb) we generate clickstream events for LocalCart and send them to Message Hub to show how data can be collected offline and streamed to the cloud later. In this notebook we listen to those clickstream events that indicate that a customer has made a purchase and  enrich these events by adding geolocation information. Using this information we aggregate the total revenue by ZIP code (for US-based transactions) or country code (for international transactions) and periodically write totals (representing revenue) to generic cloud storage (represented in this scenario by Cloud Object Storage).\n\n<img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/nb6_streams_flow.png\"></img>", "cell_type": "markdown", "metadata": {}}, {"source": "\n## Table of contents\n\n* [1.1 Import a streams flow](#import_flow) <br>\n* [1.2 Customize the streams flow](#customize_flow) <br>\n* [1.3 Run the flow](#run_flow)<br>\n* [1.4 Summary and next steps](#summary)<br>\n", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"import__flow\"></a>\n***\n\n## 1.1 Import a streams flow\n\nIn previous notebooks you've created streams flows from scratch using a wizard and manually. In this notebook you'll import and customize a streams flow that aggregates sales transactions and writes them to Cloud Object Storage.\n\nFirst\n\n1. Download https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/streams_flows/revenue_by_state_or_country.stp to your local machine. This file contains the streams flow definition you'll be working with.\n\nNext, complete the following steps in IBM Watson Data Platform:\n\n1. Select a project that you want to contain the streams flow. Note that this project must be attached to Cloud Object Storage and not Object Storage (Swift).\n1. Click the **Assets** tab and scroll to the _Streams flows_ section. \n > If no section with the name is displayed the selected project is not attached to Cloud Object Storage.\n1. Click **+ New streams flow**.\n1. In the _New Streams Flow_ window, \n  1. Select **from file**\n  1. Select an existing Streaming Analytics service or create a new one (choosing the _Lite_ plan, which is free.) \n  1. Browse to the streams flow file you've downloaded. \n   > The flow name and description are populated for you. You can change the default if desired.\n  1. Click **Create**. Wait for the import to complete.\n  \n1. Review the flow. It comprises of one source operator (Message Hub), a [Python] code operator (retrieving customer geolocation information from the Cloud), two filter operators (separating US transactions from international transactions), two aggregation operators (one for each major geography, calculating the revenue) and two Cloud Object Storage target operators, saving the aggregated data for later processing.\n \n > You'll notice that the run button is disabled, because the flow it is not yet properly configured for your environment. \n\n1. To identify the issues, click the highlighted notification icon on the right hand side.\n<img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/beta_2/notebooks/images/nb6_import_notifications.png\"></img>\n1. Click on the notification to open the canvas and expand the highlighted error list icon\n > Note that the (Message Hub) source operator and the two Cloud Object Storage target operators are tagged as invalid. This is expected because they are associated with service instances that you don't have access to.\n\n<a id=\"customize_flow\"></a>\n***\n\n## 1.2 Customize the streams flow\n\n\n#### Resolve the Message Hub operator issue\n1. Open the Message Hub operator\n > Note that no connection and no topic are assigned to it.\n1. Select your existing Message Hub connection or create a new one. \n1. From the _Topic_ dropdown select **logout_with_purchase**.\n1. Customize the schema\n  1. Detect the schema.\n  1. Change the **customer_id** attribute type from _Number_ to **Text**.\n  1. Save your changes.\n1. Save the streams flow.\n> The Source Message Hub operator should no longer be flagged as invalid.\n \n#### Resolve the Cloud Object Storage operator issues\n\n1. Open the first _Cloud Object Storage_ operator\n > Note that no connection and no file path is assigned to it because your Watson Data Platform environment is different from the environment where the flow was created.\n1. Select your existing Cloud Object Storage connection or create a new one. \n1. Customize the file path, which defines where operator will write the output to.\n  1. Open the data asset selector and select an existing bucket.\n   > Don't choose an existing object from the list. You'll specify a new generic name in the next step.\n  1. Append `/us_revenue_%TIME` to the file path, to specify the object name pattern\n   > `%TIME` will be replaced with a timestamp. Your path should look as follows: `/my-existing-bucket/us_revenue_%TIME`\n1. Review the other settings but do not make any other changes.\n > Pay attention to the file writing policy, which defines how frequently aggregated data is written to storage.\n1. Save the streams flow.\n > The first Cloud Object Storage operator should no longer be flagged as invalid.\n\n ***\n\n1. Open the second _Cloud Object Storage_ operator\n1. Select your existing Cloud Object Storage connection. \n1. Customize the file path, which defines where operator will write the output to.\n  1. Open the data asset selector and select an existing bucket.\n  1. Append `/foreign_revenue_%TIME` to the file path, to specify the object name pattern\n   > `%TIME` will be replaced with a timestamp\n1. Review the other settings but do not make any other changes.\n1. Save the streams flow.\n> The streams flow should now be valid.\n\n\n<a id=\"run_flow\"></a>\n***\n\n## 1.3 Run the flow\n\n1. Run the customized flow. After a minute or two data should be streaming from the source to the targets.\n1. Wait until at least one data file containing revenue information for US transactions and international transactions has been written to the sepcified Cloud Object Storage bucket before continuing.\n\n<a id=\"summary\"></a>\n***\n\n## 1.4 Summary and next steps\n\nYou've learned how to import and custommize a streams flow and got aquainted with the code operator, the filter operator and the aggregation operator.\n\nNext, learn how to click-stream data in Notebook 6b: TBD.\n", "cell_type": "markdown", "metadata": {}}, {"source": "\n***\n\n### Authors\n\nPatrick Titzler is a Developer Advocate for Watson Data Platform at IBM. \n\n***\nCopyright \u00a9 IBM Corp. 2017. This notebook and its source code are released under the terms of the MIT License.", "cell_type": "markdown", "metadata": {}}, {"source": "", "cell_type": "code", "metadata": {}, "execution_count": null, "outputs": []}], "metadata": {"language_info": {"pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}, "mimetype": "text/x-python", "name": "python", "file_extension": ".py", "version": "2.7.11", "nbconvert_exporter": "python"}, "kernelspec": {"language": "python", "display_name": "Python 2 with Spark 2.0", "name": "python2-spark20"}}, "nbformat_minor": 1}