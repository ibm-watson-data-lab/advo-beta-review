{"metadata": {"language_info": {"codemirror_mode": {"version": 2, "name": "ipython"}, "nbconvert_exporter": "python", "file_extension": ".py", "version": "2.7.11", "mimetype": "text/x-python", "pygments_lexer": "ipython2", "name": "python"}, "kernelspec": {"display_name": "Python 2 with Spark 2.0", "language": "python", "name": "python2-spark20"}}, "nbformat": 4, "cells": [{"metadata": {}, "cell_type": "markdown", "source": "# LocalCart scenario part 5: Build a product recommendation engine"}, {"metadata": {}, "cell_type": "markdown", "source": "In this notebook, you'll first load historical shopping data. Then you'll structure and view that data in a table that displays customer information, product categories, and shopping history details. You will use the _k_-means algorithm, which is useful for cluster analysis in data mining, to segment customers into clusters for the purpose of making an in-store purchase recommendation based on shopping history. You\u2019ll deploy the model to the IBM Watson Machine Learning service in IBM Bluemix to create your recommendation application. By the end of the notebook, you\u2019ll understand how to build a model to provide product recommendations for customers based on their purchase history.\n\n<img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/nb5_flow.png\"></img>\n\nThis notebook runs on Python 2 with Spark 2.0."}, {"metadata": {}, "cell_type": "markdown", "source": "## Table of contents\n\n1. [Setup](#setup)<br>\n    1.1. [Import libraries](#libraries)<br>\n    1.2. [Load sample data](#load)<br>\n    1.3. [View data in a table](#view_table)<br>\n2. [Create a KMeans model](#kmeans)<br>\n    2.1. [Prepare data](#prepare_data)<br>\n    2.2. [Create clusters and define the model](#build_model)<br>\n3. [Persist the model](#persist)<br>\t\n4. [Deploy the model to the cloud](#deploy)<br>\n\t4.1. [Create deployment for the model](#create_deploy)<br>\n\t4.2. [Test model deployment](#test_deploy)<br>\n5. [Create product recommendations](#create_recomm)<br>\n6. [Summary and next steps](#summary)<br>"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## 1. Setup\n\nYou need to import the required library and load the customer shopping data into this notebook.\n\nImport the PixieDust library:"}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": null, "source": "import pixiedust"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# If the previous cell caused an error:\n# (1) Uncomment the last line in this cell by removing the # sign\n# (2) Run this cell\n# (3) Restart the kernel\n# (4) Re-run the first cell \n#!pip install --user --upgrade pixiedust"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"load\"></a>\n### 1.1. Load sample data\n\nIn this section you'll load the data file that contains the customer shopping data:"}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": null, "source": "df = pixiedust.sampleData(\"https://raw.githubusercontent.com/wdp-beta/get-started/master/data/customers_orders1_opt.csv\")"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"view_table\"></a>\n### 1.3. View data in a table by using Pixiedust\n\nTo better examine and visualize the data, run the following cell to view it in a table format:"}, {"outputs": [], "metadata": {"scrolled": true, "pixiedust": {"displayParams": {"handlerId": "dataframe"}}}, "cell_type": "code", "execution_count": null, "source": "display(df)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"kmeans\"></a>\n## 2. Create a _k_-means model\n\nIn this section of the notebook you'll use the _k_-means implementation to associate every customer to a cluster based on their shopping history.\n\nFirst, import the Apache\u00ae Spark machine learning packages that you'll need in the subsequent steps:\n"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.clustering import KMeansModel\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import Vectors"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"prepare_data\"></a>\n### 2.1. Prepare data\n\nCreate a new data set with just the data that you need. Filter the columns that you want, in this case the customer ID column and the product-related columns. Remove the columns that you don't need for aggregating the data and training the model:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# Here are the product cols. In a real world scenario we would query a product table, or similar.\nproduct_cols = ['Baby Food', 'Diapers', 'Formula', 'Lotion', 'Baby wash', 'Wipes', 'Fresh Fruits', 'Fresh Vegetables', 'Beer', 'Wine', 'Club Soda', 'Sports Drink', 'Chips', 'Popcorn', 'Oatmeal', 'Medicines', 'Canned Foods', 'Cigarettes', 'Cheese', 'Cleaning Products', 'Condiments', 'Frozen Foods', 'Kitchen Items', 'Meat', 'Office Supplies', 'Personal Care', 'Pet Supplies', 'Sea Food', 'Spices']\n# Here we get the customer ID and the products they purchased\ndf_filtered = df.select(['CUST_ID'] + product_cols)"}, {"metadata": {}, "cell_type": "markdown", "source": "Run the `display()` command again, this time to view the filtered information:"}, {"outputs": [], "metadata": {"pixiedust": {"displayParams": {"handlerId": "dataframe"}}}, "cell_type": "code", "execution_count": null, "source": "display(df_filtered)"}, {"metadata": {}, "cell_type": "markdown", "source": "Now, aggregate the individual transactions for each customer to get a single score per product, per customer."}, {"outputs": [], "metadata": {"pixiedust": {"displayParams": {"handlerId": "dataframe"}}}, "cell_type": "code", "execution_count": null, "source": "df_customer_products = df_filtered.groupby('CUST_ID').sum()  # Use customer IDs to group transactions by customer and sum them up\ndf_customer_products = df_customer_products.drop('sum(CUST_ID)')\ndisplay(df_customer_products)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"build_model\"></a>\n### 2.2. Create clusters and define the model \n\nYou'll use _k_-means to create 100 clusters based on the number of times a specific customer purchased a product.\n\nFirst, create a feature vector by combining the product and quantity columns:"}, {"outputs": [], "metadata": {"pixiedust": {"displayParams": {"handlerId": "dataframe"}}, "collapsed": true}, "cell_type": "code", "execution_count": null, "source": "assembler = VectorAssembler(inputCols=[\"sum({})\".format(x) for x in product_cols],outputCol=\"features\") # Assemble vectors using product fields"}, {"metadata": {}, "cell_type": "markdown", "source": "Next, create the _k_-means clusters and the pipeline to define the model:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "kmeans = KMeans(maxIter=50, predictionCol=\"cluster\").setK(100).setSeed(1)  # Initialize model\npipeline = Pipeline(stages=[assembler, kmeans])\nmodel = pipeline.fit(df_customer_products)"}, {"metadata": {}, "cell_type": "markdown", "source": "Finally, calculate the cluster for each customer by running the original dataset against the _k_-means model: "}, {"outputs": [], "metadata": {"scrolled": false, "pixiedust": {"displayParams": {"handlerId": "dataframe"}}}, "cell_type": "code", "execution_count": null, "source": "df_customer_products_cluster = model.transform(df_customer_products)\ndisplay(df_customer_products_cluster)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"persist\"></a>\n## 3. Persist the model \n\nIn this section you will learn how to store your model in the Watson Machine Learning repository by using the Python client libraries.\n\n### 3.1 Import the necessary libraries\n\nFirst, you must import the client libraries."}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "from repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact"}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2. Configure IBM Watson Machine Learning credentials\nTo access your machine learning repository programmatically, you need to copy in your credentials, which you can see in your **IBM Watson Machine Learning** service details in Bluemix.\n\n1. Open your [Bluemix Data Services list](https://apsportal.ibm.com/settings/services?context=analytics) in a new browser window. (You can also navigate to this list by clicking the avatar icon on the upper right hand side and selecting _Settings_ and _Services_.\n2. Click on your IBM Watson Machine Learning service.\n3. Click **Service Credentials** and then click **View credentials**.\n4. Copy your credentials and paste them into the `username` and `password` fields in the next cell.\n5. Run the cell.\n"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "service_path = 'https://ibm-watson-ml.mybluemix.net'\nusername = 'copy_your_username_here'\npassword = 'copy_your_password_here'"}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.3 Authorize the repository and save artifacts\n\nAuthorize the repository client by running the following code, which uses the Watson Machine Learning client libraries:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "ml_repository_client = MLRepositoryClient(service_path)\nml_repository_client.authorize(username, password)\nml_model_name = 'Shopping History'"}, {"metadata": {}, "cell_type": "markdown", "source": "Save the model to your Watson Machine Learning instance:"}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": null, "source": "data_training = df_customer_products.withColumnRenamed('CUST_ID', 'label')\nmodel_artifact = MLRepositoryArtifact(model, training_data=data_training, name=ml_model_name)\nsaved_model = ml_repository_client.models.save(model_artifact)\nsaved_model"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"deploy\"></a>\n## 4. Deploy model to the cloud\n\nIn this section you'll learn how to deploy the model to the cloud by using the Watson Machine Learning REST API. For more information about REST APIs, see the [Watson Machine Learning API Documentation](http://watson-ml-api.mybluemix.net/).\n\nImport the required libraries:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "import json\nimport requests\nimport urllib3"}, {"metadata": {}, "cell_type": "markdown", "source": "To work with the Watson Machine Learning REST API, generate an access token:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "headers = urllib3.util.make_headers(basic_auth='{}:{}'.format(username, password))\nurl = '{}/v2/identity/token'.format(service_path)\nresponse = requests.get(url, headers=headers)\nml_token = json.loads(response.text).get('token')"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"create_deploy\"></a>\n### 4.1. Create deployment for the model\n\nNow you can create a deployment for the model in Watson Machine Learning:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "deployment_url = service_path + \"/v2/published_models/\" + saved_model.uid + \"/deployments/\"\ndeployment_header = {'Content-Type': 'application/json', 'Authorization': ml_token}\ndeployment_payload = {\"type\": \"online\", \"name\": \"Shopping List Prediction\"}\ndeployment_response = requests.post(deployment_url, json=deployment_payload, headers=deployment_header)"}, {"metadata": {}, "cell_type": "markdown", "source": "Execute the following code to load the scoring endpoint for the deployment. "}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": null, "source": "scoring_url = json.loads(deployment_response.text).get('entity').get('scoring_href')\nprint scoring_url"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"test_deploy\"></a>\n### 4.2. Test deployment of the model\n\nTo verify that the model was successfully deployed to the cloud, you'll specify a customer ID, for example customer 12027, to predict this customer's cluster against the Watson Machine Learning deployment, and see if it matches the cluster that was previously associated this customer ID."}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": null, "source": "customer = df_customer_products_cluster.filter('CUST_ID = 12027').collect()\nprint(\"Previously calculated cluster = {}\".format(customer[0].cluster))"}, {"metadata": {}, "cell_type": "markdown", "source": "To determine the customer's cluster using Watson Machine Learning, you need to load the customer's purchase history. This function uses the local data frame to select every product field and the number of times that customer 12027 purchased a product."}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "from six import iteritems\ndef get_product_counts_for_customer(cust_id):\n    cust = df_customer_products.filter('CUST_ID = 12027').take(1)\n    fields = []\n    values = []\n    for row in customer:\n        for product_col in product_cols:\n            field = 'sum({})'.format(product_col)\n            value = row[field]\n            fields.append(field)\n            values.append(value)\n    return (fields, values)"}, {"metadata": {}, "cell_type": "markdown", "source": "This function takes the customer's purchase history and calls the scoring endpoint:"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "def get_cluster_from_watson_ml(fields, values):\n    scoring_header = {'Content-Type': 'application/json', 'Authorization': ml_token}\n    scoring_payload = {'fields': fields, 'values': [values]}\n    scoring_response = requests.post(scoring_url, json=scoring_payload, headers=scoring_header)\n    return json.loads(scoring_response.text)['values'][0][len(product_cols)+1]"}, {"metadata": {}, "cell_type": "markdown", "source": "Finally, call the functions defined above to get the product history, call the scoring endpoint, and get the cluster associated to customer 12027:"}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": null, "source": "product_counts = get_product_counts_for_customer(12027)\nfields = product_counts[0]\nvalues = product_counts[1]\nprint(\"Cluster calculated by Watson ML = {}\".format(get_cluster_from_watson_ml(fields, values)))"}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.3. Optional: Delete previous models\n\nIf needed, you can delete previous deployments and models that you created using this notebook. "}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# Uncomment this cell to delete a model that was previously created\n# ml_models = ml_repository_client.models.all()\n# for ml_model in ml_models:\n#     if ml_model.name == ml_model_name:\n#         # delete any deployments\n#         deployment_header = {'Content-Type': 'application/json', 'Authorization': ml_token}\n#         deployment_url = service_path + \"/v2/published_models/\" + ml_model.uid + \"/deployments/\"\n#         deployment_response = requests.get(deployment_url, headers=deployment_header)\n#         o = json.loads(deployment_response.text)\n#         if 'resources' in o.keys():\n#             for resource in o['resources']:\n#                 deployment_url = service_path + \"/v2/published_models/\" + ml_model.uid + \"/deployments/\" + resource['metadata']['guid']\n#                 deployment_response = requests.delete(deployment_url, headers=deployment_header)\n#                 print deployment_response.text\n#         # delete the model\n#         ml_repository_client.models.remove(ml_model.uid)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"create_recomm\"></a>\n## 5. Create product recommendations\n\nNow you can create some product recommendations.\n\nFirst, run this cell to create a function that queries the database and finds the most popular clusters. In this case, the **df_customer_products_cluster** dataframe is the database."}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# This function gets the most popular clusters in the cell by grouping by the cluster column\ndef get_popular_products_in_cluster(cluster):\n    df_cluster_products = df_customer_products_cluster.filter('cluster = {}'.format(cluster))\n    df_cluster_products_agg = df_cluster_products.groupby('cluster').sum()\n    row = df_cluster_products_agg.rdd.collect()[0]\n    items = []\n    for product_col in product_cols:\n        field = 'sum(sum({}))'.format(product_col)\n        items.append((product_col, row[field]))\n    sortedItems = sorted(items, key=lambda x: x[1], reverse=True) # Sort by score\n    popular = [x for x in sortedItems if x[1] > 0]\n    return popular"}, {"metadata": {}, "cell_type": "markdown", "source": "Now, run this cell to create a function that will calculate the recommendations based on a given cluster. This function finds the most popular products in the cluster, filters out products already purchased by the customer or currently in the customer's shopping cart, and finally produces a list of recommended products."}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# This function takes a cluster and the quantity of every product already purchased or in the user's cart\nfrom pyspark.sql.functions import desc\ndef get_recommendations_by_cluster(cluster, purchased_quantities):\n    # Existing customer products\n    print('PRODUCTS ALREADY PURCHASED/IN CART:')\n    customer_products = []\n    for i in range(0, len(product_cols)):\n        if purchased_quantities[i] > 0:\n            customer_products.append((product_cols[i], purchased_quantities[i]))\n    df_customer_products = sc.parallelize(customer_products).toDF([\"PRODUCT\",\"COUNT\"])\n    df_customer_products.show()\n    # Get popular products in the cluster\n    print('POPULAR PRODUCTS IN CLUSTER:')\n    cluster_products = get_popular_products_in_cluster(cluster)\n    df_cluster_products = sc.parallelize(cluster_products).toDF([\"PRODUCT\",\"COUNT\"])\n    df_cluster_products.show()\n    # Filter out products the user has already purchased\n    print('RECOMMENDED PRODUCTS:')\n    df_recommended_products = df_cluster_products.alias('cl').join(df_customer_products.alias('cu'), df_cluster_products['PRODUCT'] == df_customer_products['PRODUCT'], 'leftouter')\n    df_recommended_products = df_recommended_products.filter('cu.PRODUCT IS NULL').select('cl.PRODUCT','cl.COUNT').sort(desc('cl.COUNT'))\n    df_recommended_products.show(10)"}, {"metadata": {}, "cell_type": "markdown", "source": "Next, run this cell to create a function that produces a list of recommended items based on the products and quantities in a user's cart. This function uses Watson Machine Learning to calculate the cluster based on the shopping cart contents and then calls the **get_recommendations_by_cluster** function."}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# This function would be used to find recommendations based on the products and quantities in a user's cart\ndef get_recommendations_for_shopping_cart(products, quantities):\n    fields = []\n    values = []\n    for product_col in product_cols:\n        field = 'sum({})'.format(product_col)\n        if product_col in products:\n            value = quantities[products.index(product_col)]\n        else:\n            value = 0\n        fields.append(field)\n        values.append(value)\n    return get_recommendations_by_cluster(get_cluster_from_watson_ml(fields, values), values)"}, {"metadata": {}, "cell_type": "markdown", "source": "Run this cell to create a function that produces a list of recommended items based on the purchase history of a customer. This function uses Watson Machine Learning to calculate the cluster based on the customer's purchase history and then calls the **get_recommendations_by_cluster** function."}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": "# This function is used to find recommendations based on the purchase history of a customer\ndef get_recommendations_for_customer_purchase_history(customer_id):\n    product_counts = get_product_counts_for_customer(customer_id)\n    fields = product_counts[0]\n    values = product_counts[1]\n    return get_recommendations_by_cluster(get_cluster_from_watson_ml(fields, values), values)"}, {"metadata": {}, "cell_type": "markdown", "source": "Now you can take customer 12027 and produce a recommendation based on that customer's purchase history:"}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": null, "source": "get_recommendations_for_customer_purchase_history(12027)"}, {"metadata": {}, "cell_type": "markdown", "source": "Now, take a sample shopping cart and produce a recommendation based on the items in the cart:"}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": null, "source": "get_recommendations_for_shopping_cart(['Diapers','Baby wash','Wine'],[1,2,1])"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n## Summary and next steps\n\nYou successfully completed this notebook! You learned how to use Watson Machine Learning for model creation and deployment.   \n\nCheck out other notebooks in this series: \n - [Generating a Kafka producer (JSON) into MessageHub](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-1.ipynb).\n - [Building the Streaming Pipeline](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-2.ipynb)\n - [Visualize streaming data in a real-time dashboard](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-4.ipynb) \n\n### Author\n**Mark Watson** is an IBM Developer Advocate whose primary responsibility is to make it easy for you to use the IBM platform and offerings. He is passionate about the three pillars of his work, namely development, advocacy, and community. While coding is still an essential part of his job, he focuses on examples, training materials, explanatory demos, Software Development Kits and tools that can help people excel at using IBM products."}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 2017 IBM. This notebook and its source code are released under the terms of the MIT License."}], "nbformat_minor": 1}